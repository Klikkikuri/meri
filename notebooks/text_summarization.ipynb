{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization\n",
    "\n",
    "To provide a context for the LLM's, we need to summarize some of the text. Usually LLM's are quite capable of this, but due to their own safeguards and biases, the summaries may not be as non-opinionated.\n",
    "\n",
    "Key requirement for Klikkikuri is:\n",
    "- Summary __MUST__ be as non-opinionated as possible, and close to the original text.\n",
    "- Summary __SHOULD__ distill the essential points of the text.\n",
    "- Summary __NEEDS__ to contain the most important points of the text.\n",
    "- Summary __NEEDS__ to contain the entities and relations of the text.\n",
    "- Summary __SHOULD__ containt the most unique points of the text for the RAG retrieval to be effective.\n",
    "- Summary __SHOULD__ be as short as possible, but not too short.\n",
    "- Summarization tool must be able to support multiple languages.\n",
    "\n",
    "## Common approaches\n",
    "\n",
    "- Extractive Summarization: Selects and extracts key sentences or phrases from the original text to create a summary. It retains the original wording and structure, making it less prone to introducing biases or opinions.\n",
    "- Abstractive Summarization: Generates a summary that may not directly quote the original text. It can rephrase and condense information, but it may introduce biases or opinions if not carefully controlled.\n",
    "- Hybrid Summarization: Combines both extractive and abstractive methods to leverage the strengths of both approaches. It can provide a more comprehensive summary while still maintaining a degree of non-opinionatedness.\n",
    "\n",
    "Maintaining objectivity requires avoiding the introduction of subjective opinions, interpretations, or biases in the summary. Simultaneously, the summary should focus on the most relevant and unique points and reflect the entities and relationships present in the original text.\n",
    "\n",
    "Abstactive methods are more prone to introducing errors or \"hallucinations\" because they generate new text rather than selecting existing sentences. Extractive methods, by relying on the original text, are less likely to introduce such errors but may not capture the essence of the text as effectively.\n",
    "\n",
    "\n",
    "## Papers\n",
    "- Abstractive Text Summarization: State of the Art, Challenges, and Improvements: https://arxiv.org/pdf/2409.02413\n",
    "- A Brief Survey on Text Summarization Methods: https://ijrpr.com/uploads/V5ISSUE3/IJRPR23824.pdf\n",
    "- Evaluation of Python Text Summarization Libraries: https://rjwave.org/ijedr/papers/IJEDR2101019.pdf\n",
    "- Text Summarization: A Bibliometric Study and Systematic Literature Review: https://www.iieta.org/download/file/fid/145253\n",
    "- A Unified Approach to Text Summarization: Classical, Machine Learning, and Deep Learning Methods: https://www.iieta.org/download/file/fid/156413\n",
    "- An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics: https://arxiv.org/pdf/2207.00939\n",
    "\n",
    "## Guides\n",
    "- A-Z Guide to Text Summarization in Python for Beginners: https://www.projectpro.io/article/text-summarization-python-nlp/546\n",
    "- Comparing Text Summarization Techniques: https://medium.com/@thakermadhav/comparing-text-summarization-techniques-d1e2e465584e\n",
    "-  LLM Summarization: Getting To Production: https://arize.com/blog/llm-summarization-getting-to-production/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "From [A Brief Survey on Text Summarization Methods](https://ijrpr.com/uploads/V5ISSUE3/IJRPR23824.pdf):\n",
    "\n",
    "a. ROUGE (Recall-Oriented Understudy for Gisting Evaluation): ROUGE measures how well the summary covers important\n",
    "information from the original text. It looks at the overlap between the summary and the reference summaries (i.e., human-generated\n",
    "summaries). ROUGE calculates several scores, such as ROUGE-N (measuring overlap in n-grams) and ROUGE-L (measuring the\n",
    "longest common subsequence) [2].\n",
    "b. BLEU (Bilingual Evaluation Understudy): BLEU evaluates the quality of a summary by comparing it to one or more reference\n",
    "summaries. It measures how many n-grams (sequences of words) in the summary match those in the reference summaries. BLEU\n",
    "scores range from 0 to 1, with higher scores indicating better quality summaries [3].\n",
    "c. METEOR (Metric for Evaluation of Translation with Explicit Ordering): METEOR assesses the overall quality of a summary by\n",
    "considering both content overlap and surface-level similarity. It takes into account word matching, word order, and stemming.\n",
    "METEOR scores range from 0 to 1, with higher scores indicating better quality summaries [4]."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
