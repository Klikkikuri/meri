{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from meri.utils import setup_logging\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "from IPython.display import display_markdown\n",
    "from meri.scraper import try_setup_requests_cache\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "try_setup_requests_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwclient.page\n",
    "from meri.wp import page_to_document\n",
    "from meri.wp import mediawiki_html_to_markdown\n",
    "import mwclient\n",
    "\n",
    "site = mwclient.Site(\"en.wikipedia.org\")\n",
    "page = site.pages[\"Pet store\"]\n",
    "\n",
    "html_doc = page_to_document(page)\n",
    "md_doc = mediawiki_html_to_markdown(html_doc)\n",
    "\n",
    "md_doc.meta['revision'] = page.revision\n",
    "\n",
    "display(md_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meri.pipelines.summation import LmmSummationPipeline\n",
    "\n",
    "display(md_doc.meta)\n",
    "\n",
    "farts = LmmSummationPipeline()(md_doc)\n",
    "display(farts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a structured tree with summaries. Summaries are generated with LLM\n",
    "# TODO: Find alternatives for LLM summarization\n",
    "\n",
    "SUMMARY_PROMPT = \"\"\"\n",
    "You are a helpful AI assistant for Klikkikuri service for generating a comprehensive summary of a Wikipedia article.\n",
    "\n",
    "Your task is to summarize the given text in the <article> -section. Aim for the following:\n",
    "- Summary __MUST__ be as non-opinionated as possible, and close to the original text.\n",
    "- Summary __MUST__ be in the same language as the article.\n",
    "- Summary __MAY NOT__ contain any additional information, context or commentary in the summary that is not in the text.\n",
    "- Summary __NEEDS__ to contain the entities and relations of the text.\n",
    "- Summary __NEEDS__ to contain the most important points of the text.\n",
    "- Summary __SHOULD__ distill the essential points of the text.\n",
    "- Summary __SHOULD__ be as short as feasible.\n",
    "{# - Summary __SHOULD__ contain the most unique points of the text of the section. #}\n",
    "- Summary __MAY ONLY__ include basic semantic formatting in markdown like bold or emphasis, but __MAY NOT__ include formatting like links, images, tables, headings, etc.\n",
    "- Approach the task as extractive task, but format it as a abstract summary.\n",
    "- Do __NOT__ add *any* preceding sentences (like \"This is a summary of ...\", \"The section ...\") or trailing sentences.\n",
    "- The output should begin immediately with the summarized content.\n",
    "- For sections listing sources or references, return `{{SKIP_TAG}}`.\n",
    "- If no sensible summary can be generated, return `{{SKIP_TAG}}`.\n",
    "\n",
    "You are to summarize following wikipedia article titled {{article_title|escape}}.\n",
    "Section of the article to summarize is {{section_title|escape}} – keep the summary relevant to it.\n",
    "Subsectioned content inside of `<summary>` -tags are previously summarized sections for context – focus on providing additional information to them, but do not repeat content that is provided already on them.\n",
    "\n",
    "{# You MUST only produce the summary, no yapping, no explanations. #}\n",
    "\n",
    "Article to summarize in markdown format:\n",
    "<article>\n",
    "{{text|indent}}\n",
    "</article>\n",
    "\"\"\"\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Tuple\n",
    "from haystack import Pipeline, component\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from meri.llm import get_generator\n",
    "from meri.wp import MarkdownChunker\n",
    "\n",
    "\n",
    "def reduce_to_summary(md: str, lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Reduce the given markdown text to a summary using the OpenAI LLM.\n",
    "\n",
    "    Parse the markdown text into a tree structure, and then recursively summarize each node in the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    MAX_CONCURRENT_WORKERS: int = 3\n",
    "    SKIP_TAG = \"<skip>\"\n",
    "    doc_tree = MarkdownChunker(md, language=lang).parse()\n",
    "\n",
    "    prompt_builder = PromptBuilder(SUMMARY_PROMPT)\n",
    "    llm = get_generator()\n",
    "\n",
    "    p = Pipeline(max_runs_per_component=1)\n",
    "    p.add_component(\"prompt_builder\", prompt_builder)\n",
    "    p.add_component(\"llm\", llm)\n",
    "    p.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    def make_node_text(node, titles):\n",
    "        section_titles = titles + [node['title']]\n",
    "        section_titles_md = \"\\n\\n\".join(f\"#{'#' * i} {_title}\" for i, _title in enumerate(section_titles))\n",
    "        return f\"{section_titles_md}\\n\\n{node['body']}\", section_titles\n",
    "\n",
    "    @lru_cache(maxsize=128)\n",
    "    def summarize_node_sync(text, titles) -> str:\n",
    "        results = p.run({\n",
    "            \"prompt_builder\": {\n",
    "                \"text\": text,\n",
    "                \"article_title\": titles[0],\n",
    "                \"section_title\": titles[-1],\n",
    "                \"SKIP_TAG\": SKIP_TAG,\n",
    "            },\n",
    "        })\n",
    "        if \"llm\" in results:\n",
    "            return results[\"llm\"][\"replies\"][0]\n",
    "        raise RuntimeError(\"No summary generated\")\n",
    "\n",
    "    \n",
    "    def generate_summary(node, titles, executor, futures_map):\n",
    "        print(f\"{' '*len(titles)}|- node: {node['title']}\")\n",
    "        new_titles = titles + [node[\"title\"]]\n",
    "        node_text, full_titles = make_node_text(node, titles)\n",
    "\n",
    "        if node['children']:\n",
    "            # Recurse first (sequentially), collect child summaries later\n",
    "            for child in node['children']:\n",
    "                generate_summary(child, new_titles, executor, futures_map)\n",
    "\n",
    "            # After all children are scheduled, combine summaries\n",
    "            subsection_summaries = []\n",
    "            for child in node['children']:\n",
    "                child_summary = child.get(\"summary\")\n",
    "                subsection_summaries.append(child_summary)\n",
    "\n",
    "            print(f\" >{'-'*len(titles)} {len(subsection_summaries)} summaries for section: {node['title']}\")\n",
    "            sections = [node_text]\n",
    "            subsection_titles = (f\"#{'#' * (len(new_titles) + 1)} {_title}\" for _title in titles[1:])\n",
    "            for title, summary in zip(subsection_titles, subsection_summaries):\n",
    "                if summary.strip() == SKIP_TAG:\n",
    "                    logger.debug(f\"Skipping summary for {title} due to SKIP_TAG\")\n",
    "                    continue\n",
    "\n",
    "                sections += [f\"{title}\\n\\n<summary>{summary}</summary>\"]\n",
    "            node_text = \"\\n\\n\".join(sections)\n",
    "\n",
    "        # Submit summarization task\n",
    "        future = executor.submit(summarize_node_sync, node_text, tuple(new_titles))\n",
    "        futures_map[future] = node\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_WORKERS) as executor:\n",
    "        futures_map = {}\n",
    "        generate_summary(doc_tree, [], executor, futures_map)\n",
    "\n",
    "        for future in as_completed(futures_map):\n",
    "            node = futures_map[future]\n",
    "            res = future.result()\n",
    "            if res.strip() == SKIP_TAG:\n",
    "                logger.debug(f\"Skipping summary for {node['title']} due to SKIP_TAG\")\n",
    "                continue\n",
    "            node['summary'] = res\n",
    "\n",
    "    return doc_tree\n",
    "\n",
    "summarized_docs = reduce_to_summary(md_doc.content, lang=\"en\")\n",
    "\n",
    "#display_markdown(reduce_to_summary(docs[0].content, lang=\"en\"), raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert parsed tree into documents, where Document contains a leading summaries and section body.\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Iterator\n",
    "from haystack import Document\n",
    "\n",
    "def _get_branch(tree, branches_path: List[int]) -> Iterator:\n",
    "    node = tree\n",
    "    yield node\n",
    "    for branch in branches_path: \n",
    "        node = node['children'][branch]\n",
    "        yield node\n",
    "\n",
    "\n",
    "def tree_to_doc(tree, base_doc: Document, branches_path: List[int] = []):\n",
    "    \"\"\"\n",
    "    Convert the tree structure into a list of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    branches = list(_get_branch(tree, branches_path))\n",
    "    # Collect summaries\n",
    "    content = []\n",
    "    for branch in branches[:-1]:\n",
    "        content.append(f\"{'#' * branch['level']} {branch['title']}\\n\\n{branch['summary']}\")\n",
    "    \n",
    "    # Get leaf full body\n",
    "    branch = branches[-1]\n",
    "    content.append(f\"{'#' * branch['level']} {branch['title']}\\n\\n{branch['body']}\")\n",
    "\n",
    "    doc = deepcopy(base_doc)\n",
    "    doc.content = \"\\n\\n\".join(content).strip()\n",
    "    doc.meta['title'] += f\" > {branch['title']}\"\n",
    "    doc._create_id()\n",
    "    if doc.content:\n",
    "        docs.append(doc)\n",
    "\n",
    "    # Recurse into child\n",
    "    for i, lead in enumerate(branch['children']):\n",
    "        docs += tree_to_doc(tree, doc, branches_path + [i])\n",
    "    return docs\n",
    "\n",
    "base_doc = deepcopy(md_doc)\n",
    "\n",
    "base_doc.meta['title'] = \"Wikipedia\"\n",
    "\n",
    "docs = tree_to_doc(summarized_docs, base_doc)\n",
    "print(\"Extracted document len:\", len(docs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
