{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia page history evaluation\n",
    "\n",
    "Wikipedia pages can be vandalized. Wikipedia itself prioritizes having low amount of false positives from automatic checkers, which can cause the latest version to be vandalized. Try to find a stable – but recent –  version of the page to be used as a context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach testing – Editor history\n",
    "\n",
    "Use editor edit-history to see if the user can be considered a \"trustworthy\".\n",
    "\n",
    "Recent edits are collected from page, and every editor is checked how their recent edits have been reverted.\n",
    "\n",
    "TODO: implement checks that allows self-reverts.\n",
    "TODO: Check that revert is not reverted back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests ratelimit\n",
    "import requests\n",
    "import ratelimit\n",
    "try:\n",
    "    import requests_cache\n",
    "    # get system temp directory\n",
    "    import tempfile\n",
    "    import os\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    requests_cache.install_cache(os.path.join(temp_dir, \"wp-api-cache\"))\n",
    "except ImportError:\n",
    "    print(\"No requests cache available\")\n",
    "    pass\n",
    "\n",
    "# Disable hugginface stats\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = \"1\"\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# revision tags that are used to indicate revision is reverted by later edit.\n",
    "# https://en.wikipedia.org/wiki/Special:Tags\n",
    "MW_REVERTED_TAGS: set[str] = {\"mw-reverted\"}\n",
    "\n",
    "_session = requests.Session()\n",
    "\n",
    "@ratelimit.sleep_and_retry\n",
    "@ratelimit.limits(calls=5, period=1)\n",
    "def wpapi(params, headers={}, site=\"en.wikipedia.org\"):\n",
    "    url = f\"https://{site}/w/api.php\"\n",
    "    params.setdefault(\"format\", \"json\")\n",
    "    response = _session.get(url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    if \"error\" in data:\n",
    "        raise Exception(f\"WPAPI Error: {data[\"error\"]}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting revert\n",
    "\n",
    "Wikipedia doesn't contain reliable labeled information that can be used to indicate if the edit is a reverted. Sometimes the edit might contain tags as indicator, sometimes not. We can use the edit comment to check if the edit is a reverting previous work. If the comment contains keywords such as \"revert\", \"undid\", or \"rv\" plus the user's name or revision number, we can consider the edit as a revert.\n",
    "\n",
    "TODO: Check if the comment mentions that it's reverting to an earlier commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Function to detect if the edit is a revert\n",
    "\n",
    "import re\n",
    "from typing import TypedDict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "revert_phrases = [\n",
    "    \"Reverted edits by {user!r}\",\n",
    "    \"Reverted 1 pending edit by [[Special:Contributions/{user}|{user}]] to revision <number> by <username>: <reason description>\",\n",
    "    \"Reverting possible vandalism by {user!r} to version by <username>\",  # Cluebot NG\n",
    "    \"Reverted edit by [[Special:Contribs/{user}|{user}]] ([[User talk:{user}|talk]]) to last version by <username>\",\n",
    "    \"Undid revision [[Special:Diff/{revid}|{revid}]] by [[Special:Contributions/{user}|{user}]] ([[User talk:{user}|talk]]): <reason description>\",\n",
    "    # \"Reverted edit \",\n",
    "    # \"Undid revision\",\n",
    "    # \"Restored revision <number> by <username>\",\n",
    "    # \"Revert\",\n",
    "    # \"Rollback\",\n",
    "    # \"Undo\",\n",
    "    # \"Reverted\"\n",
    "]\n",
    "\n",
    "class PageRevision(TypedDict, total=False):\n",
    "    pageid: int\n",
    "    revid: int\n",
    "    parentid: int\n",
    "    title: str\n",
    "    user: str\n",
    "    timestamp: str\n",
    "    comment: str\n",
    "    tags: list[str]\n",
    "    site: str = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def is_revert(revision: PageRevision, original_edit, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Indicate if the edit is a revert.\n",
    "\n",
    "    Uses sentence-transformers to semantically compare the comment with\n",
    "    predefined revert indicator phrases.\n",
    "\n",
    "    Returns True if the comment is likely a revert.\n",
    "    \"\"\"\n",
    "\n",
    "    log = logger.getChild(\"is_revert\")\n",
    "    log.debug(\"Checking if revision %r is a revert\", revision)\n",
    "\n",
    "    comment = revision.get(\"comment\", \"\")\n",
    "    # Remove comment (/* ... */) from the comment that is used to indicate section\n",
    "    comment = re.sub(r'/\\* .*? \\*/', '', comment)\n",
    "\n",
    "    if not comment:\n",
    "        log.debug(\"Revision %r has no comment, skipping\", revision)\n",
    "        return False\n",
    "\n",
    "    # If the revision ID or username is not mentioned, it's not likely a revert. \n",
    "    username = original_edit.get(\"user\", \"\")\n",
    "    revision_id_str = str(original_edit.get(\"revid\"))\n",
    "\n",
    "    rev_pattern = r'\\b' + re.escape(revision_id_str) + r'\\b'\n",
    "    username_pattern = r'\\b' + re.escape(username) + r'\\b'\n",
    "    \n",
    "    has_revision_id = bool(re.search(rev_pattern, comment))\n",
    "    user_mentioned = bool(re.search(username_pattern, comment))\n",
    "\n",
    "    if not any((has_revision_id, user_mentioned)):\n",
    "        log.debug(\"Revision ID %s or username %r not mentioned in comment: %r\", revision_id_str,  username, comment)\n",
    "        return False\n",
    "\n",
    "    formatted_revert_phrases = [s.format(**original_edit) for s in revert_phrases]\n",
    "\n",
    "    # Encode the revert phrases.\n",
    "    revert_embeddings = model.encode(formatted_revert_phrases, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    # Encode the comment.\n",
    "    comment_embedding = model.encode(comment, convert_to_tensor=True, show_progress_bar=False)\n",
    "    # Compute (cosine) similarity with the pre-encoded revert phrases.\n",
    "    cosine_scores = model.similarity(comment_embedding, revert_embeddings)\n",
    "\n",
    "    # Use the maximum similarity score as the indicator.\n",
    "    # Using for-loop to be able to log the similarity score for each pattern.\n",
    "    #max_score = cosine_scores.max().item()\n",
    "    max_score = 0.0\n",
    "    for i, score in enumerate(cosine_scores[0]):\n",
    "        max_score = max(score, max_score)\n",
    "        log.debug(f\" {max_score:.2f} - Pattern {formatted_revert_phrases[i]!r} match to {comment!r}\")\n",
    "\n",
    "    # Set a threshold for similarity (this needs to be fine-tuned).\n",
    "    if max_score >= threshold:\n",
    "        log.info(\"Revision %r is a revert\", revision)\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'revid': 1254954349,\n",
       "  'parentid': 1254925063,\n",
       "  'user': 'Binksternet',\n",
       "  'timestamp': '2024-11-02T13:09:40Z',\n",
       "  'comment': 'Reverted 1 edit by [[Special:Contributions/31.217.4.176|31.217.4.176]] ([[User talk:31.217.4.176|talk]]) to last revision by 162 etc.',\n",
       "  'tags': ['mw-undo', 'twinkle', 'mw-reverted'],\n",
       "  'pageid': '49079438'},\n",
       " {'revid': 1254925063,\n",
       "  'parentid': 1254811923,\n",
       "  'user': '31.217.4.176',\n",
       "  'anon': '',\n",
       "  'timestamp': '2024-11-02T10:02:53Z',\n",
       "  'comment': '',\n",
       "  'tags': ['mobile edit',\n",
       "   'mobile web edit',\n",
       "   'visualeditor',\n",
       "   'mw-reverted',\n",
       "   'disambiguator-link-added'],\n",
       "  'pageid': '49079438'},\n",
       " {'revid': 1254811923,\n",
       "  'parentid': 1254792543,\n",
       "  'user': '162 etc.',\n",
       "  'timestamp': '2024-11-01T20:19:03Z',\n",
       "  'comment': 'Undid revision [[Special:Diff/1254792543|1254792543]] by [[Special:Contributions/31.217.4.176|31.217.4.176]] ([[User talk:31.217.4.176|talk]])',\n",
       "  'tags': ['mw-undo', 'wikieditor'],\n",
       "  'pageid': '49079438'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_last_revisions(page_title, limit=20) -> list[PageRevision]:\n",
    "    \"\"\"\n",
    "    Fetches the last `limit` revisions for the given Wikipedia page title.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'titles': page_title,\n",
    "        'rvlimit': limit,\n",
    "        'rvprop': 'ids|timestamp|user|comment|tags',\n",
    "    }\n",
    "\n",
    "    data = wpapi(params)\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    revisions = []\n",
    "    # The query returns a dictionary keyed by pageid\n",
    "    for page_id, page in pages.items():\n",
    "        if \"missing\" in page:\n",
    "            print(f\"The page '{page_title}' does not exist on Wikipedia.\")\n",
    "            return None\n",
    "        revisions = page.get('revisions', [])\n",
    "        revisions.extend([PageRevision(**rev, pageid=page_id) for rev in revisions])\n",
    "    return revisions\n",
    "\n",
    "page_edits = fetch_last_revisions(\"Sofi_Tukker\", limit=15)\n",
    "display(page_edits[-3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 3 contributions by '162 etc.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'userid': 41625025,\n",
       "  'user': '162 etc.',\n",
       "  'pageid': 17756493,\n",
       "  'revid': 1277183439,\n",
       "  'parentid': 1274694448,\n",
       "  'ns': 0,\n",
       "  'title': 'List of countries by oil production',\n",
       "  'timestamp': '2025-02-23T05:09:11Z',\n",
       "  'comment': '',\n",
       "  'tags': ['mw-undo', 'wikieditor']},\n",
       " {'userid': 41625025,\n",
       "  'user': '162 etc.',\n",
       "  'pageid': 37250174,\n",
       "  'revid': 1277183385,\n",
       "  'parentid': 1275214248,\n",
       "  'ns': 0,\n",
       "  'title': 'List of countries by silver production',\n",
       "  'timestamp': '2025-02-23T05:08:35Z',\n",
       "  'comment': 'Undid revision [[Special:Diff/1275214248|1275214248]] by [[Special:Contributions/Ousama27|Ousama27]] ([[User talk:Ousama27|talk]])',\n",
       "  'tags': ['mw-undo', 'wikieditor']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_user_contributions(username: str, limit=100) -> list[PageRevision]:\n",
    "    \"\"\"\n",
    "    Fetches up to `limit` contributions made by the given user.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"usercontribs\",\n",
    "        \"ucuser\": username,\n",
    "        \"uclimit\": limit,\n",
    "        \"ucprop\": \"ids|title|timestamp|comment|tags\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params=params)\n",
    "\n",
    "    # Only consider the page edits\n",
    "    return [PageRevision(edit) for edit in data.get(\"query\", {}).get(\"usercontribs\", []) if edit.get(\"ns\", 0) == 0]\n",
    "\n",
    "username = page_edits[-1]['user']\n",
    "user_contributions = fetch_user_contributions(username, limit=3)\n",
    "print(f\"Last 3 contributions by {username!r}\")\n",
    "display(user_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking edit: {'revid': 1254811923, 'parentid': 1254792543, 'user': '162 etc.', 'timestamp': '2024-11-01T20:19:03Z', 'comment': 'Undid revision [[Special:Diff/1254792543|1254792543]] by [[Special:Contributions/31.217.4.176|31.217.4.176]] ([[User talk:31.217.4.176|talk]])', 'tags': ['mw-undo', 'wikieditor'], 'pageid': '49079438'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "def get_next_revisions(revision: PageRevision, n) -> list[PageRevision]:\n",
    "    \"\"\"\n",
    "    Fetches the next `n` revisions after the given timestamp on the specified page.\n",
    "    \"\"\"\n",
    "\n",
    "    revision_timestamp = revision['timestamp']\n",
    "\n",
    "    dt = datetime.strptime(revision_timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    dt_next = dt + timedelta(seconds=1)\n",
    "    start_timestamp = dt_next.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"pageids\": revision['pageid'],\n",
    "        \"rvstart\": start_timestamp,\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvlimit\": n,\n",
    "        \"rvprop\": \"ids|timestamp|comment|tags\"\n",
    "    }\n",
    "    data = wpapi(params)\n",
    "    logger.debug(\"Next revisions: %r\", data)\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    revisions_list = []\n",
    "    for pid, revision in pages.items():\n",
    "        revisions = revision.get(\"revisions\", [])\n",
    "        revisions_list.extend([PageRevision(**rev, pageid=pid) for rev in revisions])\n",
    "    return revisions_list\n",
    "\n",
    "\n",
    "def check_contribution_reverted(revision: PageRevision, num_later_edits=10) -> List[bool | PageRevision]:\n",
    "    \"\"\"\n",
    "    Has the contribution been reverted?\n",
    "\n",
    "    Checks if a particular contribution was reverted by examining the next\n",
    "    `num_later_edits` revisions on the page. If any of those revisions' comments\n",
    "    indicate a revert targeting the given username or revision, returns True.\n",
    "\n",
    "    Returns list of N>0 if the edit is reverted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the edit has the \"mw-reverted\" tag.\n",
    "    cx_tags = MW_REVERTED_TAGS & set(revision.get(\"tags\", []))\n",
    "    logger.debug(\"Revision tags: %r\", revision.get(\"tags\", []))\n",
    "    if len(cx_tags):\n",
    "        logger.info(\"[REVERTED] Revision %d is tagged as reverted with tag(s) %r\", revision['revid'], cx_tags)\n",
    "        return [True]\n",
    "\n",
    "    # Check the follow-up revisions if they mention this edit.\n",
    "    next_revs = get_next_revisions(revision, num_later_edits)\n",
    "    logger.debug(\"Number of following revisions: %d\", len(next_revs))\n",
    "\n",
    "    r = []\n",
    "    for followup_revision in next_revs:\n",
    "        comment = followup_revision.get(\"comment\", \"\")\n",
    "\n",
    "        logger.debug(f\"Checking revision {followup_revision['revid']} with comment: {comment!r}\")\n",
    "\n",
    "        if is_revert(followup_revision, revision):\n",
    "            # TODO: We should recurse to see, if reverting revision has been reverted\n",
    "            logger.info(\"[REVERTED] Comment %r is reverted by %r (revid:%d)\", revision['comment'], comment, followup_revision['revid'])\n",
    "            r.append(followup_revision)\n",
    "    return r\n",
    "\n",
    "earliest_contrib = page_edits[-1]\n",
    "print(\"Checking edit:\", earliest_contrib)\n",
    "\n",
    "reverts = list(check_contribution_reverted(earliest_contrib))\n",
    "\n",
    "display(reverts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[REVERTED] Revision 1256966548 is tagged as reverted with tag(s) {'mw-reverted'}\n",
      "INFO:__main__:[REVERTED] Revision 1256962846 is tagged as reverted with tag(s) {'mw-reverted'}\n",
      "INFO:__main__:[REVERTED] Revision 1256962795 is tagged as reverted with tag(s) {'mw-reverted'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revert rate for 2A05:4F44:1701:2500:D010:2C76:10A4:52C7: 100.00% (3/3)\n"
     ]
    }
   ],
   "source": [
    "# Final run - Collect edits, collect editors from the edits, and check how many of the lastest edits by the editor is reverted.\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class RevertStats(NamedTuple):\n",
    "    rate: float\n",
    "    total: int\n",
    "    reverted: int\n",
    "\n",
    "def compute_user_revert_rate(username: str, contrib_limit: int = 100, later_edits_to_check: int = 10) -> RevertStats:\n",
    "    \"\"\"\n",
    "    Computes the revert rate for a user by checking up to `contrib_limit`\n",
    "    of their contributions. For each contribution, it checks the next\n",
    "    `later_edits_to_check` revisions to see if it was reverted.\n",
    "    \n",
    "    Returns a tuple: (revert_rate, total_checked, total_reverted).\n",
    "    \"\"\"\n",
    "    contributions = fetch_user_contributions(username, limit=contrib_limit)\n",
    "    if not contributions:\n",
    "        return RevertStats(None, 0, 0)\n",
    "    total = 0\n",
    "    reverted = 0\n",
    "    for contrib in contributions:\n",
    "        total += 1\n",
    "        if check_contribution_reverted(contrib, num_later_edits=later_edits_to_check):\n",
    "            reverted += 1\n",
    "        # time.sleep(0.2)\n",
    "    rate = (reverted / total) if total > 0 else 0\n",
    "    return RevertStats(rate, total, reverted)\n",
    "\n",
    "username = \"2A05:4F44:1701:2500:D010:2C76:10A4:52C7\"\n",
    "rate, total, reverted = compute_user_revert_rate(username, 10)\n",
    "print(f\"Revert rate for {username}: {rate:.2%} ({reverted}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rank the editors by revert rate\n",
    "\n",
    "editors = set([edit['user'] for edit in page_edits])\n",
    "\n",
    "# revert_rates = {editor: compute_user_revert_rate(editor) for editor in editors}\n",
    "\n",
    "# sorted_revert_rates = sorted(revert_rates.items(), key=lambda x: x[1].rate, reverse=True)\n",
    "\n",
    "# for editor, stats in sorted_revert_rates:\n",
    "#     print(f\"{editor}: {stats.rate:.2%} ({stats.reverted}/{stats.total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group based power-level\n",
    "\n",
    "Wikipedia uses RBAC (Role Based Access Control) to determine the rights of the user. In theory, the rights should have been granted by their merit. Use the groups as indicator of \"power-level\" of the user. More powerful the user is, the more likely they are to be trusted – according to wikipedia community.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:User 'Jayson Cesar Bautista' is blocked: '[[WP:SOCK|Sock puppetry]]'\n",
      "INFO:__main__:IP '2A05:4F44:1701:2500:D010:2C76:10A4:52C7' is blocked: '[[WP:Disruptive editing|Disruptive editing]], block evasion, vandalism, see 2A05:4F44:1704:D500:0:0:0:0/64'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2A05:4F44:1701:2500:D010:2C76:10A4:52C7: -10000\n",
      "31.217.4.176: 0\n",
      "107.116.79.140: 0\n",
      "Binksternet: 21\n",
      "162 etc.: 21\n",
      "174.92.221.85: 0\n",
      "GreenC bot: 11\n",
      "Chetsford: 101\n",
      "Arjayay: 21\n",
      "MusikBot II: 111\n",
      "Duckmather: 21\n",
      "Jayson Cesar Bautista: -10000\n"
     ]
    }
   ],
   "source": [
    "# Map of groups to an power level.\n",
    "\n",
    "from collections import OrderedDict\n",
    "import ipaddress\n",
    "from typing import Iterable\n",
    "\n",
    "# Group to use if the IP address is blocked.\n",
    "USER_BLOCKED_GROUP = \"blocked\"\n",
    "\n",
    "GROUP_TRUST_LEVELS = {\n",
    "    \"banned\": -10000,\n",
    "    \"blocked\": -10000,\n",
    "    \"bot\": 10,\n",
    "    \"autoconfirmed\": 1,\n",
    "    \"extendedconfirmed\": 20,\n",
    "    \"rollback\": 50,\n",
    "    \"sysop\": 100,\n",
    "    \"bureaucrat\": 400,\n",
    "    \"checkuser\": 350,\n",
    "    \"oversight\": 350,\n",
    "    \"steward\": 350,\n",
    "}\n",
    "\n",
    "def check_ip_blocked(ip: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the IP address is blocked on Wikipedia.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"blocks\",\n",
    "        \"bkip\": ip,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params)\n",
    "    block = data.get(\"query\", {}).get(\"blocks\", [])\n",
    "    if bool(block):\n",
    "        logger.info(\"IP %r is blocked: %r\", ip, block[0].get(\"reason\", \"\"))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_power_levels(users: Iterable[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get the power levels of the given users.\n",
    "\n",
    "    The power level is a sum of the trust levels of the groups the user is in. Higher\n",
    "    power level indicates more trust. The trust levels are defined in the GROUP_TRUST_LEVELS\n",
    "    dictionary.\n",
    "\n",
    "    :param users: List of usernames or IP addresses.\n",
    "    :return: List of power levels for each user.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the user map with 0 trust level.\n",
    "    user_map = OrderedDict((u, 0) for u in users)\n",
    "\n",
    "    # Separate IP addresses from usernames, they need to be queried differently.\n",
    "    ips, usernames = [], []\n",
    "    for entry in users:\n",
    "        try:\n",
    "            if ipaddress.ip_address(entry):\n",
    "                ips.append(str(entry))\n",
    "        except ValueError:\n",
    "            usernames.append(entry)\n",
    "\n",
    "    # Query the user groups and blockinfo for the given usernames.\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"users\",\n",
    "        \"ususers\": \"|\".join(usernames),\n",
    "        \"usprop\": \"groups|blockinfo\",\n",
    "    }\n",
    "    data = wpapi(params)\n",
    "\n",
    "    for user_info in data.get(\"query\", {}).get(\"users\", []):\n",
    "        # If user is tagged as \"invalid\"\n",
    "        if 'invalid' in user_info:\n",
    "            logger.warning(\"User %r is invalid: %r\", user_info.get('name', \"\"), user_info.get('invalidreason', \"\"))\n",
    "            continue\n",
    "\n",
    "        total_trust = 0\n",
    "        user = user_info['name']\n",
    "        groups = user_info.get(\"groups\", [])\n",
    "\n",
    "        if user_info.get(\"blockid\", None) is not None:\n",
    "            logger.info(\"User %r is blocked: %r\", user, user_info.get('blockreason', \"\"))\n",
    "            groups.append(USER_BLOCKED_GROUP)\n",
    "\n",
    "        for group in set(groups):\n",
    "            group_key = group.lower()  # Ensure case-insensitive matching.\n",
    "            trust = GROUP_TRUST_LEVELS.get(group_key, 0)  # Default to 0 if not mapped.\n",
    "            total_trust += trust\n",
    "        user_map[user] = total_trust\n",
    "\n",
    "    # Check if the IP address is blocked.\n",
    "    for ip in ips:\n",
    "        if check_ip_blocked(ip):\n",
    "            user_map[ip] += GROUP_TRUST_LEVELS[USER_BLOCKED_GROUP]\n",
    "\n",
    "    return list(user_map.values())\n",
    "\n",
    "def rank_by_power_level(users: Iterable[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Rank the users by their power levels.\n",
    "\n",
    "    :param users: List of usernames or IP addresses.\n",
    "    :return: List of usernames sorted by power level.\n",
    "    \"\"\"\n",
    "    power_levels = get_power_levels(users)\n",
    "    return [u for p, u in sorted(zip(power_levels, users), reverse=True) if p > 0]\n",
    "\n",
    "level_check_users = list(editors) + [\"Jayson Cesar Bautista\"]\n",
    "\n",
    "power_levels = get_power_levels(level_check_users)\n",
    "for editor, power in zip(level_check_users, power_levels):\n",
    "    print(f\"{editor}: {power}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review log\n",
    "\n",
    "Rank the revisions based on their review status. Some wikipedias employ review flags to indicate that the revision has been reviewed by a trusted user. Example from Finnish wikipedia: https://fi.wikipedia.org/wiki/Ohje:Sivujen_arviointi\n",
    "\n",
    "Having a review flag is NOT a guarantee that the information is correct, but it can be used as an indicator that the revision has no obvious vandalism.\n",
    "\n",
    "Revisions with flags are preferred, and reviews from other users are ranked above having a \"trusted\" status. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'revid': 22889573,\n",
       "  'parentid': 22889569,\n",
       "  'user': 'Fribaaja',\n",
       "  'timestamp': '2024-12-11T13:24:01Z',\n",
       "  'flagged': {'user': 'Osmo Lundell',\n",
       "   'timestamp': '2025-02-20T21:58:19Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22638561,\n",
       "  'parentid': 22595899,\n",
       "  'user': '87.95.32.17',\n",
       "  'anon': '',\n",
       "  'timestamp': '2024-09-16T19:46:57Z',\n",
       "  'flagged': {'user': 'BladeJ',\n",
       "   'timestamp': '2024-10-06T19:56:57Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22595899,\n",
       "  'parentid': 22595855,\n",
       "  'user': 'Kuosmanono',\n",
       "  'timestamp': '2024-08-25T22:41:38Z',\n",
       "  'flagged': {'user': 'BladeJ',\n",
       "   'timestamp': '2024-09-07T13:21:10Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22358375,\n",
       "  'parentid': 22273183,\n",
       "  'user': 'SaMSUoM',\n",
       "  'timestamp': '2024-04-29T16:50:05Z',\n",
       "  'flagged': {'user': 'Seegge',\n",
       "   'timestamp': '2024-07-28T20:44:51Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22079023,\n",
       "  'parentid': 22037750,\n",
       "  'user': '176.72.103.70',\n",
       "  'anon': '',\n",
       "  'timestamp': '2023-12-30T10:26:47Z',\n",
       "  'flagged': {'user': 'Elastul',\n",
       "   'timestamp': '2024-01-01T08:22:44Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22595855,\n",
       "  'parentid': 22358375,\n",
       "  'user': 'SaMSUoM',\n",
       "  'timestamp': '2024-08-25T22:17:42Z',\n",
       "  'flagged': {'user': 'SaMSUoM',\n",
       "   'timestamp': '2024-08-25T22:17:42Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22148856,\n",
       "  'parentid': 22146625,\n",
       "  'user': 'Puppe100',\n",
       "  'timestamp': '2024-01-29T15:32:53Z',\n",
       "  'flagged': {'user': 'Puppe100',\n",
       "   'timestamp': '2024-01-29T15:33:19Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22145999,\n",
       "  'parentid': 22145278,\n",
       "  'user': 'Elastul',\n",
       "  'timestamp': '2024-01-28T15:22:21Z',\n",
       "  'flagged': {'user': 'Elastul',\n",
       "   'timestamp': '2024-01-28T15:22:21Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22037750,\n",
       "  'parentid': 21747700,\n",
       "  'user': 'Unkka T. Kumiankka',\n",
       "  'timestamp': '2023-12-09T14:02:05Z',\n",
       "  'flagged': {'user': 'Unkka T. Kumiankka',\n",
       "   'timestamp': '2023-12-09T14:02:05Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REVIEWED_TAGS = {\"stable\", \"reviewed\"}\n",
    "\n",
    "def get_page_revisions_with_review(page_title, rvlimit=20):\n",
    "    \"\"\"\n",
    "    Retrieves the latest revisions for a page along with flagged (review) info.\n",
    "   \n",
    "    Revisions with \"stable\" are ranked above non-reviewed revisions, and revisions made by other users are ranked above\n",
    "    the user's own revisions.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": page_title,\n",
    "        \"rvlimit\": rvlimit,\n",
    "        \"rvdir\": \"older\",\n",
    "        \"rvprop\": \"ids|timestamp|user|flagged\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params, site=\"fi.wikipedia.org\")\n",
    "\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    r = []\n",
    "    for page_id, page in pages.items():\n",
    "        revisions = page.get(\"revisions\", [])\n",
    "        \n",
    "        for rev in revisions:\n",
    "            score = 0\n",
    "            flagged = rev.get(\"flagged\", {})\n",
    "            if not flagged: continue\n",
    "            if flagged.get(\"level_text\", \"\") in REVIEWED_TAGS:\n",
    "                score += 1\n",
    "                if rev[\"user\"] != flagged.get(\"user\", None):\n",
    "                    # If the revision is reviewed and the reviewer is different from the editor, give extra points.\n",
    "                    score += 1\n",
    "\n",
    "            r.append((score, PageRevision(rev, pageid=page_id)))\n",
    "    # Sort by score in descending order.\n",
    "    r.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [rev for score, rev in r]\n",
    "\n",
    "page_title = \"Niklas_Anttila\"\n",
    "page_revisions = get_page_revisions_with_review(page_title)\n",
    "display(page_revisions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "Using the basis presented in the paper \"Context-aware Detection of Sneaky Vandalism on Wikipedia across Multiple Languages\", use part-of-speech tagging to detect \"sneaky vandalism\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "Compute a changed sentences from the revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vandalized 'Moninaisuus, yhdenvertaisuus ja osallisuus' by '86.60.161.18' at 2025-02-05T14:53:25Z\n",
      "Previous notable revision by 'Toivo ja Toivo' at 2025-02-02T14:53:02Z\n"
     ]
    }
   ],
   "source": [
    "def load_revision(oldid: int, site=\"en.wikipedia.org\") -> PageRevision:\n",
    "    \"\"\"\n",
    "    Load the revision by :param oldid: and return the :class:`PageRevision`.\n",
    "    \"\"\"\n",
    "    oldid = int(oldid)\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"revids\": oldid,\n",
    "        \"rvprop\": \"ids|timestamp|user|comment|tags\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params, site=site)\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for page_id, page in pages.items():\n",
    "        revisions = page.get(\"revisions\", [])\n",
    "\n",
    "        assert len(revisions) == 1, f\"Expected 1 revision, got {len(revisions)}\"\n",
    "\n",
    "        return PageRevision(revisions[0], site=site, **page)\n",
    "\n",
    "    raise ValueError(f\"Revision with oldid {oldid!r} not found\")\n",
    "\n",
    "def pervious_notable_revision(revision: PageRevision) -> PageRevision:\n",
    "    \"\"\"\n",
    "    Get previous notable revision\n",
    "\n",
    "    Vandals might do a multiple edits in succession, so find a earlier revision that is from a different user, or a day before.\n",
    "    \"\"\"\n",
    "    MAX_DAYS = timedelta(days=1)\n",
    "    start_timestamp = datetime.strptime(revision['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\") + timedelta(seconds=1)\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"pageids\": revision['pageid'],\n",
    "        \"rvstart\": start_timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"rvdir\": \"older\",\n",
    "        \"rvlimit\": 20,\n",
    "        \"rvprop\": \"ids|timestamp|comment|tags|user\"\n",
    "    }\n",
    "\n",
    "    data = wpapi(params, site=revision['site'])\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for pid, page in pages.items():\n",
    "        revisions = page.get(\"revisions\", [])\n",
    "        for rev in revisions:\n",
    "            r = PageRevision(rev, site=revision['site'], **page)\n",
    "            if rev['user'] != revision['user']:\n",
    "                return r\n",
    "            rev_timestamp = datetime.strptime(rev['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            if rev_timestamp > (start_timestamp + MAX_DAYS):\n",
    "                return r\n",
    "    logger.warning(\"No notable revision found for %d (revid %r)\", revision[\"pageid\"], revision['revid'])\n",
    "    return None\n",
    "\n",
    "current_rev = load_revision(23006899, site=\"fi.wikipedia.org\")\n",
    "prev_rev = pervious_notable_revision(current_rev)\n",
    "\n",
    "print(f\"Vandalized {current_rev['title']!r} by {current_rev['user']!r} at {current_rev['timestamp']}\")\n",
    "print(f\"Previous notable revision by {prev_rev['user']!r} at {prev_rev['timestamp']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:meri.settings.settings:Values: {'DEBUG': True, 'LOGGING_LEVEL': 'DEBUG', 'llms': [{'name': 'o1-mini', 'provider': 'openai'}], 'BOT_USER_AGENT': 'Mozilla/5.0 (compatible; Klikkikuri/0.1.0; +https://github.com/Klikkikuri)'}\n",
      "INFO:meri.settings.settings:Provider to class: {'openai': <class 'meri.settings.llms.OpenAISettings'>, 'mistral': <class 'meri.settings.llms.MistralSettings'>, 'together': <class 'meri.settings.llms.TogetherAiSettings'>}\n",
      "INFO:meri.settings.settings:Settings: [OpenAISettings(name='o1-mini', provider='openai', api_key='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', model=None, api_base_url=None, temperature=0.0)]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from meri.scraper import get_user_agent\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"trafilatura\").setLevel(logging.DEBUG)\n",
    "\n",
    "def wp_page(revision: PageRevision) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the Wikipedia page content for the given revision.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"oldid\": revision[\"revid\"]}\n",
    "\n",
    "    params.update({\n",
    "        \"action\": \"parse\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"displaytitle|text|headhtml\",\n",
    "        \"disablelimitreport\": True,\n",
    "        \"disableeditsection\": True,\n",
    "        \"mobileformat\": False,\n",
    "        \"contentmodel\": \"wikitext\",\n",
    "        \"disabletoc\": True,\n",
    "    })\n",
    "\n",
    "    site = \"fi.wikipedia.org\"\n",
    "    data = wpapi(params, site=site)\n",
    "\n",
    "    perm_url = \"https://{site}/w/index.php?title={title}&oldid={revid}\".format(\n",
    "        site=site,\n",
    "        **data[\"parse\"]\n",
    "    )\n",
    "\n",
    "    html = \"\\n\".join([\n",
    "        data[\"parse\"][\"headhtml\"][\"*\"],\n",
    "        '<main id=\"content\" class=\"mw-body\"><div id=\"bodyContent\" class=\"content\">',\n",
    "        f'<h1>{data['parse']['displaytitle']}</h1>',\n",
    "        data[\"parse\"][\"text\"][\"*\"],\n",
    "        '</div></main>',\n",
    "        '</body></html>'\n",
    "    ])\n",
    "\n",
    "    from trafilatura import load_html\n",
    "    from trafilatura.settings import DEFAULT_CONFIG\n",
    "    from trafilatura.htmlprocessing import prune_unwanted_nodes\n",
    "    from lxml.etree import XPath\n",
    "    config = deepcopy(DEFAULT_CONFIG)\n",
    "    config[\"DEFAULT\"].setdefault(\"USER_AGENTS\", get_user_agent())\n",
    "\n",
    "    doc = load_html(html)\n",
    "\n",
    "    # Convert <a> tags to text\n",
    "    for a in doc.xpath(\"//a\"):\n",
    "        a.tag = \"span\"\n",
    "        a.text = a.text or \"\"\n",
    "        a.attrib.clear()\n",
    "\n",
    "    # Prune unwanted nodes\n",
    "    doc = prune_unwanted_nodes(doc, [XPath(x) for x in [\n",
    "        \"//script\", \"//noscript\", \"//style\", \"//link\", \"//meta\", \"//form\", \"//input\", \"//button\",\n",
    "        '//*[contains(@class, \"noprint\") or contains(@class, \"ambox-notice\")]',  # Remove \"noprint\" classed content\n",
    "    ]])\n",
    "\n",
    "    #from lxml import etree\n",
    "    #print(etree.tostring(doc).decode())\n",
    "\n",
    "    # TODO: Use trafilatura bare_extraction\n",
    "    #from trafilatura import bare_extraction\n",
    "    from trafilatura.htmlprocessing import tree_cleaning, convert_tags\n",
    "    from trafilatura.core import Extractor\n",
    "    from trafilatura.xml import xmltotxt\n",
    "    options = Extractor(\n",
    "        config=config,\n",
    "        output_format=\"txt\",\n",
    "        formatting=False,\n",
    "        links=False,\n",
    "        images=False,\n",
    "        tables=True,\n",
    "        comments=False,\n",
    "    )\n",
    "\n",
    "    doc = tree_cleaning(doc, options)\n",
    "    doc = convert_tags(doc, options, perm_url)\n",
    "    txt = xmltotxt(doc.body, False)\n",
    "\n",
    "    # Cleaup hacks\n",
    "    # Remove citation links\n",
    "    txt = re.sub(r'\\[\\s*\\d+\\]', '', txt)\n",
    "    # Clean whitespace before word boundaries, left by some inline tags from trafilatura (e.g. <i> <b>)\n",
    "    txt = re.sub(r'\\s+([.,;:!?)])', r'\\1', txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "page_text = wp_page(current_rev)\n",
    "\n",
    "#display_markdown(page_text[:300], raw=True)\n",
    "#print(page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Aligned sentences:** 204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "V: `'Marc-André ter Stegen (s. 30. huhtikuuta 1992 Mönchengladbach, Nordrhein-Westfalenin osavaltio, Saksa) on saksalainen jalkapallomaalivahti, joka edustaa Espanjan La Ligass'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "C: `'Marc-André ter Stegen (s. 30. huhtikuuta 1992 Mönchengladbach, Nordrhein-Westfalenin osavaltio, Saksa) on saksalainen jalkapallomaalivahti, joka edustaa Espanjan La Ligassa pelaavaa FC Barcelonaa.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "V: `'↑ Marc-André Ter Stegen FC Barcelona.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Clean: N/A**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "V: `'Viitattu 21.3.2015.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Clean: N/A**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "V: `'↑ Saksalainen huippulupaus Barcelonan maalille Sportti.com.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "C: `'a b Saksalainen huippulupaus Barcelonan maalille Sportti.com.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "V: `None`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "C: `'↑ Marc-André Ter Stegen FC Barcelona.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "V: `None`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "C: `'Viitattu 21.3.2015.'`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import difflib\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "import spacy\n",
    "import spacy.tokens\n",
    "#spacy.cli.download(\"fi_core_news_md\")\n",
    "nlp = spacy.load(\"fi_core_news_md\")\n",
    "\n",
    "class SentrenceAligner:\n",
    "    \"\"\"\n",
    "    Handles text preprocessing, sentence segmentation, token-level alignment,\n",
    "    and feature extraction using spaCy.\n",
    "    \"\"\"\n",
    "\n",
    "    nlp: spacy.Language\n",
    "\n",
    "\n",
    "    def __init__(self, nlp_model):\n",
    "        self.similarity_treshold = 0.6  # Minimum similarity for sentence alignment.\n",
    "        self.nlp = nlp_model\n",
    "\n",
    "\n",
    "    def extract_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Uses spaCy to segment markdown text into sentences.\n",
    "        \"\"\"\n",
    "        # Split into paragraphs, at least the Finnish model does not handle newlines well\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\")]\n",
    "        sentences = []\n",
    "        for para in paragraphs:\n",
    "            doc = self.nlp(para)\n",
    "            sentences.extend([sent.text.strip() for sent in doc.sents if sent.text.strip()])\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def sentence_similarity(self, sent1: str, sent2: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two sentences using both Levenshtein Distance and word embeddings.\n",
    "        \"\"\"\n",
    "        sent1 = sent1.strip()\n",
    "        sent2 = sent2.strip()\n",
    "        if not sent1 or not sent2:\n",
    "            logger.debug(\"Empty sentence: %r, %r\", sent1, sent2)\n",
    "            return 0.0\n",
    "\n",
    "        from rapidfuzz import fuzz\n",
    "        fuzz_sim = fuzz.ratio(sent1, sent2) / 100.0\n",
    "        \n",
    "        # # Compute cosine similarity using word embeddings (optional, requires spaCy model with vectors)\n",
    "        # doc1, doc2 = self.nlp(sent1), self.nlp(sent2)\n",
    "        # if doc1.vector_norm and doc2.vector_norm:\n",
    "        #     embedding_sim = doc1.similarity(doc2)\n",
    "        # else:\n",
    "        #     # Use sentence-transformers as a fallback if spaCy model lacks word vectors\n",
    "        #     sent1_vec = model.encode(sent1, convert_to_tensor=True, show_progress_bar=False)\n",
    "        #     sent2_vec = model.encode(sent2, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "        #     embedding_sim = model.similarity(sent1_vec, sent2_vec)\n",
    "        #     embedding_sim = embedding_sim.item()\n",
    "        \n",
    "        # Combine levenshtein similarity and word embeddings similarity\n",
    "        # return (0.6 * fuzz_sim) + (0.4 * embedding_sim)\n",
    "        return fuzz_sim\n",
    "\n",
    "\n",
    "    def align_documents(self, clean_text: str, vandal_text: str) -> List[Tuple[Optional[str], Optional[str]]]:\n",
    "        \"\"\"\n",
    "        Align sentences between the non-vandalized (clean) and vandalized documents.\n",
    "        \n",
    "        Returns a list of tuples: (vandalized_sentence, corresponding_clean_sentence or None).\n",
    "        \"\"\"\n",
    "        clean_sents = self.extract_sentences(clean_text)\n",
    "        vandal_sents = self.extract_sentences(vandal_text)\n",
    "\n",
    "        matcher = difflib.SequenceMatcher(None, clean_sents, vandal_sents)\n",
    "        aligned = []\n",
    "        unmatched_clean_sents = set(range(len(clean_sents)))  # Track unmatched clean sentences\n",
    "\n",
    "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "            if tag == 'equal':\n",
    "                # Direct one-to-one sentence mapping\n",
    "                for idx_clean, idx_vandal in zip(range(i1, i2), range(j1, j2)):\n",
    "                    aligned.append((vandal_sents[idx_vandal], clean_sents[idx_clean]))\n",
    "                    unmatched_clean_sents.discard(idx_clean)\n",
    "\n",
    "            elif tag == 'replace':\n",
    "                # Try to find best matches between replaced sentences\n",
    "                for idx_vandal in range(j1, j2):\n",
    "                    best_match_idx = None\n",
    "                    best_sim = 0\n",
    "                    for idx_clean in range(i1, i2):\n",
    "                        sim = self.sentence_similarity(vandal_sents[idx_vandal], clean_sents[idx_clean])\n",
    "                        if sim > best_sim and sim > self.similarity_treshold:  # Threshold to avoid false matches\n",
    "                            best_sim = sim\n",
    "                            best_match_idx = idx_clean\n",
    "                    \n",
    "                    if best_match_idx is not None:\n",
    "                        aligned.append((vandal_sents[idx_vandal], clean_sents[best_match_idx]))\n",
    "                        unmatched_clean_sents.discard(best_match_idx)\n",
    "                    else:\n",
    "                        aligned.append((vandal_sents[idx_vandal], None))  # No match found\n",
    "\n",
    "            elif tag == 'insert':\n",
    "                # New sentences in vandalized version (likely vandalism)\n",
    "                for idx in range(j1, j2):\n",
    "                    aligned.append((vandal_sents[idx], None))\n",
    "\n",
    "            elif tag == 'delete':\n",
    "                # Sentences missing in vandalized version (potential removals)\n",
    "                for idx in range(i1, i2):\n",
    "                    aligned.append((None, clean_sents[idx]))  # Keep track of removed sentences\n",
    "                    unmatched_clean_sents.discard(idx)\n",
    "\n",
    "        # Handle possible sentence splits/merges\n",
    "        aligned = self.handle_sentence_merging(aligned)\n",
    "\n",
    "        return aligned\n",
    "\n",
    "    def handle_sentence_merging(self, aligned: List[Tuple[Optional[str], Optional[str]]]) -> List[Tuple[Optional[str], Optional[str]]]:\n",
    "        \"\"\"\n",
    "        Handles cases where a sentence may have been split or merged by vandalism.\n",
    "        If a sequence of sentences have high similarity when merged, treat them as a single unit.\n",
    "        \"\"\"\n",
    "        merged_aligned = []\n",
    "        buffer = []\n",
    "\n",
    "        for idx, (vandal, clean) in enumerate(aligned):\n",
    "            if vandal is None or clean is None:\n",
    "                buffer.append((vandal, clean))\n",
    "                continue\n",
    "            \n",
    "            if buffer:\n",
    "                merged_vandal = \" \".join(filter(None, [x[0] for x in buffer]))\n",
    "                merged_clean = \" \".join(filter(None, [x[1] for x in buffer]))\n",
    "\n",
    "                # If merging sentences improves similarity, use the merged version\n",
    "                if self.sentence_similarity(merged_vandal, merged_clean) > self.sentence_similarity(vandal, clean):\n",
    "                    merged_aligned.append((merged_vandal, merged_clean))\n",
    "                else:\n",
    "                    merged_aligned.extend(buffer)\n",
    "\n",
    "                buffer = []\n",
    "\n",
    "            merged_aligned.append((vandal, clean))\n",
    "\n",
    "        if buffer:\n",
    "            merged_aligned.extend(buffer)\n",
    "\n",
    "        return merged_aligned\n",
    "\n",
    "\n",
    "# Load vandalized version\n",
    "# 22951005\n",
    "vandalized_rev = load_revision(18823241, site=\"fi.wikipedia.org\")\n",
    "\n",
    "# Find unvandalized version\n",
    "clean_rev = pervious_notable_revision(vandalized_rev)\n",
    "\n",
    "vandalized_text = wp_page(vandalized_rev)\n",
    "clean_text = wp_page(clean_rev)\n",
    "\n",
    "fe = SentrenceAligner(nlp)\n",
    "aligned = fe.align_documents(clean_text, vandalized_text)\n",
    "display_markdown(f\"**Aligned sentences:** {len(aligned)}\", raw=True)\n",
    "\n",
    "for vandal_sent, clean_sent in aligned:\n",
    "    if vandal_sent == clean_sent:\n",
    "        #display_markdown(f\"**Identical:** {vandal_sent}\", raw=True)\n",
    "        continue\n",
    "    display_markdown(f\"V: `{vandal_sent!r}`\", raw=True)\n",
    "    if clean_sent:\n",
    "        display_markdown(f\"C: `{clean_sent!r}`\", raw=True)\n",
    "    else:\n",
    "        display_markdown(\"**Clean: N/A**\", raw=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Aligned sentences:** 6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FeatureExtractor(SentrenceAligner):\n",
    "    \n",
    "    NORMAL = 'N'\n",
    "    VANDALIZED = 'V'\n",
    "\n",
    "    class WordFeature(TypedDict):\n",
    "        \"\"\"\n",
    "        Features for a single token in a sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        bias: float  = 1.0\n",
    "        token_lower: str\n",
    "        token_lemma: str\n",
    "        token_pos: str\n",
    "        token_tag: str\n",
    "        token_shape: str\n",
    "        is_alpha: bool\n",
    "        is_digit: bool\n",
    "\n",
    "        # Features of previous token\n",
    "        prev_token_lower: Optional[str] = None\n",
    "        prev_token_pos: Optional[str] = None\n",
    "        prev_token_tag: Optional[str] = None\n",
    "\n",
    "        # Features of next token\n",
    "        next_token_lower: Optional[str] = None\n",
    "        next_token_pos: Optional[str] = None\n",
    "        next_token_tag: Optional[str] = None\n",
    "\n",
    "        # Special features for beginning and end of sentence\n",
    "        EOS: bool = False\n",
    "        BOS: bool = False\n",
    "\n",
    "\n",
    "    def token_level_label(self, vandalized_sentence: str, clean_sentence: Optional[str]) -> List[Tuple[spacy.tokens.Token, str]]:\n",
    "        \"\"\"\n",
    "        Given a vandalized sentence and its corresponding clean sentence (if available),\n",
    "        return a list of (token, label) pairs.\n",
    "        \n",
    "        If c_sent is provided, use difflib.SequenceMatcher on token lists to label tokens:\n",
    "        - tokens inserted or replaced (relative to c_sent) are labeled 'V'\n",
    "        - tokens unchanged are labeled 'N'\n",
    "        \n",
    "        If c_sent is None (i.e. sentence is extra in vandalized text), label all tokens as 'V'.\n",
    "\n",
    "        :return: List of (token, label) pairs for the vandalized sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        doc_v = self.nlp(vandalized_sentence)\n",
    "        v_tokens = self.tokenize_sentence(vandalized_sentence)\n",
    "        \n",
    "        if clean_sentence is None:\n",
    "            logger.info(\"Empty clean sentence, marking full sentence to be vandalized\")\n",
    "            # Entire sentence considered vandalized\n",
    "            return [(token, self.VANDALIZED) for token in doc_v]\n",
    "\n",
    "        labels = [self.NORMAL] * len(v_tokens)  # Mark as clean by default\n",
    "\n",
    "        # Early stop: Both sentences are same, so sentence is \"clean\"\n",
    "        if vandalized_sentence == clean_sentence:\n",
    "            logger.debug(\"Sentences are identical, returning as NORMAL\")\n",
    "            return list(zip(v_tokens, labels))\n",
    "\n",
    "        c_tokens = self.tokenize_sentence(clean_sentence)\n",
    "        \n",
    "        matcher = difflib.SequenceMatcher(None, c_tokens, v_tokens)\n",
    "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "            # TODO: Cleanup\n",
    "            if tag == 'equal':\n",
    "                for j in range(j1, j2):\n",
    "                    labels[j] = self.NORMAL\n",
    "            elif tag in ('replace', 'insert'):\n",
    "                for j in range(j1, j2):\n",
    "                    labels[j] = self.VANDALIZED\n",
    "            elif tag == 'delete':\n",
    "                # 'delete' doesn't affect tokens in the vandalized sentence.\n",
    "                pass\n",
    "            else:\n",
    "                logger.warning(\"Unknown tag: %r\", tag)\n",
    "        return [(token, labels[idx]) for idx, token in enumerate(doc_v)]\n",
    "\n",
    "\n",
    "    def word_to_features(self, words: List[spacy.tokens.Token], i: int) -> \"WordFeature\":\n",
    "        \"\"\"\n",
    "        Extract features for the token at index i in a sentence.\n",
    "        Includes token features and context from neighboring tokens.\n",
    "        \"\"\"\n",
    "        token = words[i]\n",
    "\n",
    "        # TODO: Maybe add distance from the beginning and end of the sentence as a feature\n",
    "        features = self.WordFeature(\n",
    "            bias=1.0,\n",
    "            token_lower=token.text.lower(),\n",
    "            token_lemma=token.lemma_,\n",
    "            token_pos=token.pos_,\n",
    "            token_tag=token.tag_,\n",
    "            token_shape=token.shape_,\n",
    "            is_alpha=token.is_alpha,\n",
    "            is_digit=token.is_digit,\n",
    "        )\n",
    "\n",
    "        # Add features from neighboring tokens\n",
    "        if i > 0:\n",
    "            token_prev = words[i - 1]\n",
    "            features.update({\n",
    "                'prev_token_lower': token_prev.text.lower(),\n",
    "                'prev_token_pos': token_prev.pos_,\n",
    "                'prev_token_tag': token_prev.tag_,\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True  # Beginning of sentence\n",
    "        if i < len(words) - 1:\n",
    "            token_next = words[i + 1]\n",
    "            features.update({\n",
    "                'next_token_lower': token_next.text.lower(),\n",
    "                'next_token_pos': token_next.pos_,\n",
    "                'next_token_tag': token_next.tag_,\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True  # End of sentence\n",
    "        return features\n",
    "\n",
    "\n",
    "    def tokenize_sentence(self, sentence: str) -> List[spacy.tokens.Token]:\n",
    "        doc = self.nlp(sentence)\n",
    "        return [token for token in doc]\n",
    "\n",
    "\n",
    "    def sentence_to_features(self, sent: List[spacy.tokens.Token]) -> List[\"WordFeature\"]:\n",
    "        \"\"\"\n",
    "        Converts a list of spaCy tokens into a list of feature dictionaries.\n",
    "        \"\"\"\n",
    "        return [self.word_to_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "    def extract_features_and_labels(self, aligned_sentences: Iterable[Tuple[str, Optional[str]]]) -> Tuple[List[spacy.tokens.Token], List[str]]:\n",
    "        \"\"\"\n",
    "        Given a vandalized revision (text_v) and a non-vandalized revision (text_nv),\n",
    "        align sentences and produce token-level feature sequences and corresponding\n",
    "        label sequences.\n",
    "        Returns X (list of feature lists) and y (list of label lists).\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for sent_v, sent_c in aligned_sentences:            \n",
    "            # Require vandalized sentence to be present\n",
    "            if not sent_v:\n",
    "                logger.warning(\"Empty vandalized sentence, skipping\")\n",
    "                continue\n",
    "\n",
    "            tokens, labels = [], []\n",
    "            for token, label in self.token_level_label(sent_v, sent_c):\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "\n",
    "            X.append(self.sentence_to_features(tokens))\n",
    "            y.append(labels)\n",
    "        return X, y\n",
    "\n",
    "# Extract features for the aligned sentences\n",
    "fe = FeatureExtractor(nlp)\n",
    "\n",
    "# Remove sentences that are identical\n",
    "aligned = [(vandal, clean) for vandal, clean in aligned if vandal != clean]\n",
    "display_markdown(f\"**Aligned sentences:** {len(aligned)}\", raw=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54918b4ccb1481a80a08e562ab6c8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading revisions:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Loaded 32 revisions**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "training_revisions = [\n",
    "    22951005,   # Poliittinen korrektius\n",
    "    23006899,   # Moninaisuus, yhdenvertaisuus ja osallisuus\n",
    "    23042124,   # Norja\n",
    "    22661118,   # Venäjä\n",
    "    23029995,   # Venäjä\n",
    "    23042115,   # Lannoite\n",
    "    22987743,   # Aku Ankka\n",
    "    22617727,   # Aku Ankka\n",
    "    22942673,   # Kansallinen Kokoomus\n",
    "    22377099,   # Kansallinen Kokoomus\n",
    "    23041181,   # Lionel Messi\n",
    "    22367461,   # Vallankumous\n",
    "    23040144,   # Salla_Simukka, sotkua\n",
    "    23039811,   # Siirtolaisuus_Yhdysvaltoihin, sotkua\n",
    "    23039446,   # Japanilainen_musiikki, sotkua\n",
    "    23038754,   # Junes_Lokka, sotkua\n",
    "    23037992,   # Tino_(etunimi)\n",
    "    23037938,\n",
    "    22356605,\n",
    "    19532112,\n",
    "    18823241,\n",
    "    23007534,\n",
    "    22992090,\n",
    "    22479326,\n",
    "    22927673,\n",
    "    22942743,\n",
    "    22751169,\n",
    "    22276305,\n",
    "    22987592,\n",
    "    22027199,\n",
    "    22436312,\n",
    "    9751045,\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "progress = tqdm(training_revisions, desc=\"Loading revisions\")\n",
    "# Load the revisions\n",
    "for rev_id in progress:\n",
    "    progress.set_description(f\"Loading revision {rev_id}\")\n",
    "    rev = load_revision(rev_id, site=\"fi.wikipedia.org\")\n",
    "    prev_rev = pervious_notable_revision(rev)\n",
    "\n",
    "    progress.set_description(f\"Retrieving {rev['title']} by {rev['user']}\")\n",
    "    vandal_text = wp_page(rev)\n",
    "    clean_text = wp_page(prev_rev)\n",
    "\n",
    "    dataset.append((vandal_text, clean_text))\n",
    "\n",
    "display_markdown(f\"**Loaded {len(dataset)} revisions**\", raw=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>CRF</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    max_iterations=100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class CRFClassifier:\n",
    "    def __init__(self, nlp_model, crf_params=None):\n",
    "        if crf_params is None:\n",
    "            crf_params = {\n",
    "                'algorithm': 'lbfgs',\n",
    "                'c1': 0.1,\n",
    "                'c2': 0.1,\n",
    "                'max_iterations': 100,\n",
    "                'all_possible_transitions': True\n",
    "            }\n",
    "        self.crf = sklearn_crfsuite.CRF(**crf_params)\n",
    "        self.fe = FeatureExtractor(nlp_model=nlp_model)\n",
    "\n",
    "\n",
    "    def train(self, document_pairs: List[Tuple[str, str]]):\n",
    "        \"\"\"\n",
    "        Train the CRF model.\n",
    "        :param document_pairs: A list of tuples, each (nonvandalized_text, vandalized_text)\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = self._prepare_data(document_pairs)\n",
    "\n",
    "        # Count the number of tokens in each class\n",
    "        token_labels = Counter([label for labels in y for label in labels])\n",
    "        logger.info(\"%d token labels: %r\", len(X), token_labels)\n",
    "\n",
    "        assert len(X) == len(y)\n",
    "\n",
    "        return self._train(X, y)\n",
    "\n",
    "    def _prepare_data(self, document_pairs: List[Tuple[str, str]]):\n",
    "        X_train, y_train = [], []\n",
    "\n",
    "        progress = tqdm(document_pairs, desc=\"Preprocessing and aligning sentences\")\n",
    "\n",
    "        # Preprocess and align sentences\n",
    "        for vandalized, clean in progress:\n",
    "            progress.set_description(f\"Aligning sentences for {len(vandalized)} characters\")\n",
    "\n",
    "            aligned = self.fe.align_documents(clean, vandalized)\n",
    "    \n",
    "            # Remove sentences that are identical\n",
    "            aligned = [(vandal, clean) for vandal, clean in aligned if vandal != clean]\n",
    "\n",
    "            if len(aligned) == 0:\n",
    "                logger.warning(\"No changed sentences found\")\n",
    "                continue\n",
    "\n",
    "            progress.set_description(f\"Extracting features from {len(aligned)} aligned sentences\")\n",
    "            X, y = self.fe.extract_features_and_labels(aligned)\n",
    "            X_train.extend(X)\n",
    "            y_train.extend(y)\n",
    "\n",
    "            # Add negative examples\n",
    "            negative_training_pairs = [(clean, clean) for _, clean in aligned if clean is not None]\n",
    "            X, y = self.fe.extract_features_and_labels(negative_training_pairs)\n",
    "            logger.debug(\"Added %d clean samples from negative sampling\", len(X))\n",
    "\n",
    "            X_train.extend(X)\n",
    "            y_train.extend(y)\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        return self.crf.fit(X, y)\n",
    "\n",
    "\n",
    "    def predict(self, text: str) -> List[Tuple[List, List[str]]]:\n",
    "        \"\"\"\n",
    "        Predict token-level labels for each sentence in the input text.\n",
    "        Returns a list of tuples: (sentence, predicted_labels)\n",
    "        \"\"\"\n",
    "        sentences = self.fe.extract_sentences(text)\n",
    "        predictions = []\n",
    "        for sent in sentences:\n",
    "            tokens = self.fe.tokenize_sentence(sent)\n",
    "            features = self.fe.sentence_to_features(tokens)\n",
    "\n",
    "            pred_labels = self.crf.predict_single(features)\n",
    "            predictions.append((features, pred_labels))\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def evaluate_text(self, text: str) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate the CRF model using the input text, assuming the text is a non-vandalized version.\n",
    "        \"\"\"\n",
    "        sentences = self.fe.extract_sentences(text)\n",
    "        X, y = self.fe.extract_features_and_labels([sentences, sentences])\n",
    "        return self.evaluate(X, y)\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test: List[List[dict]], y_test: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        Evaluate the CRF model using the test data.\n",
    "\n",
    "        :param X_test: List of feature lists.\n",
    "        :param y_test: List of label lists.\n",
    "        \"\"\"\n",
    "        y_pred = self.crf.predict(X_test)\n",
    "        f1_score = metrics.flat_f1_score(y_test, y_pred, average='weighted')\n",
    "        classification_report = metrics.flat_classification_report(y_test, y_pred)\n",
    "        logger.info(f\"F1 Score: {f1_score}\")\n",
    "        logger.info(\"Classification Report:\\n\" + classification_report)\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save the trained CRF model to disk using pickle.\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.crf, f)\n",
    "        logger.info(\"Model saved to %s\", filename)\n",
    "\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Load a CRF model from disk.\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.crf = pickle.load(f)\n",
    "        logger.info(\"Model loaded from %s\", filename)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "crf = CRFClassifier(nlp)\n",
    "\n",
    "try:\n",
    "    # Cache hack, don't preprocess if not necessary\n",
    "    X\n",
    "    y\n",
    "except:\n",
    "    X, y = crf._prepare_data(dataset)\n",
    "\n",
    "# Train the CRF model\n",
    "crf._train(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viimeaikainen V\n",
      "kehitys V\n",
      "on V\n",
      "kuitenkin V\n",
      "harventanut V\n",
      "dei-politiikkaa V\n",
      "soveltavien V\n",
      "yritysten V\n",
      "ja V\n",
      "organisaatioiden V\n",
      "määrää V\n",
      "yhdysvalloissa V\n",
      "ja V\n",
      "lisännyt V\n",
      "saavutuksiin V\n",
      "perustuvaa V\n",
      "( V\n",
      "merit V\n",
      "based V\n",
      ") V\n",
      "tukemista V\n",
      ", V\n",
      "jonka V\n",
      "puitteissa V\n",
      "myöskään V\n",
      "enemmistöryhmiin V\n",
      "kuuluvia V\n",
      "henkilöitä V\n",
      "ei V\n",
      "syrjitä V\n",
      ". V\n"
     ]
    }
   ],
   "source": [
    "# From X sample texts\n",
    "import random\n",
    "X_test, *_ = random.sample(X, 1)\n",
    "\n",
    "test_sentence = \" \".join(s['token_lower'] for s in X_test)\n",
    "for _sent in crf.predict(test_sentence):\n",
    "    for _token, _label in zip(_sent[0], _sent[1]):\n",
    "        print( _token['token_lower'], _label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
