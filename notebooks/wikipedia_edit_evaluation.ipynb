{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia page history evaluation\n",
    "\n",
    "Wikipedia pages can be vandalized. Wikipedia itself prioritizes having low amount of false positives from automatic checkers, which can cause the latest version to be vandalized. Try to find a stable – but recent –  version of the page to be used as a context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach testing – Editor history\n",
    "\n",
    "Use editor edit-history to see if the user can be considered a \"trustworthy\".\n",
    "\n",
    "Recent edits are collected from page, and every editor is checked how their recent edits have been reverted.\n",
    "\n",
    "TODO: implement checks that allows self-reverts.\n",
    "TODO: Check that revert is not reverted back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests ratelimit\n",
    "import requests\n",
    "import ratelimit\n",
    "try:\n",
    "    import requests_cache\n",
    "    requests_cache.install_cache(\"/tmp/wp-api-cache\")\n",
    "except ImportError:\n",
    "    print(\"No requests cache available\")\n",
    "    pass\n",
    "\n",
    "# Disable hugginface stats\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = \"1\"\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# revision tags that are used to indicate revision is reverted by later edit.\n",
    "# https://en.wikipedia.org/wiki/Special:Tags\n",
    "MW_REVERTED_TAGS: set[str] = {\"mw-reverted\"}\n",
    "\n",
    "_session = requests.Session()\n",
    "\n",
    "@ratelimit.sleep_and_retry\n",
    "@ratelimit.limits(calls=5, period=1)\n",
    "def wpapi(params, headers={}, site=\"en.wikipedia.org\"):\n",
    "    url = f\"https://{site}/w/api.php\"\n",
    "    params.setdefault(\"format\", \"json\")\n",
    "    response = _session.get(url, params=params, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    if \"error\" in data:\n",
    "        raise Exception(f\"WPAPI Error: {data[\"error\"]}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting revert\n",
    "\n",
    "Wikipedia doesn't contain reliable labeled information that can be used to indicate if the edit is a reverted. Sometimes the edit might contain tags as indicator, sometimes not. We can use the edit comment to check if the edit is a reverting previous work. If the comment contains keywords such as \"revert\", \"undid\", or \"rv\" plus the user's name or revision number, we can consider the edit as a revert.\n",
    "\n",
    "TODO: Check if the comment mentions that it's reverting to an earlier commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Function to detect if the edit is a revert\n",
    "\n",
    "import re\n",
    "from typing import TypedDict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
    "\n",
    "revert_phrases = [\n",
    "    \"Reverted edits by {user!r}\",\n",
    "    \"Reverted 1 pending edit by [[Special:Contributions/{user}|{user}]] to revision <number> by <username>: <reason description>\",\n",
    "    \"Reverting possible vandalism by {user!r} to version by <username>\",  # Cluebot NG\n",
    "    \"Reverted edit by [[Special:Contribs/{user}|{user}]] ([[User talk:{user}|talk]]) to last version by <username>\",\n",
    "    \"Undid revision [[Special:Diff/{revid}|{revid}]] by [[Special:Contributions/{user}|{user}]] ([[User talk:{user}|talk]]): <reason description>\",\n",
    "    # \"Reverted edit \",\n",
    "    # \"Undid revision\",\n",
    "    # \"Restored revision <number> by <username>\",\n",
    "    # \"Revert\",\n",
    "    # \"Rollback\",\n",
    "    # \"Undo\",\n",
    "    # \"Reverted\"\n",
    "]\n",
    "\n",
    "class PageRevision(TypedDict, total=False):\n",
    "    pageid: int\n",
    "    revid: int\n",
    "    parentid: int\n",
    "    user: str\n",
    "    timestamp: str\n",
    "    comment: str\n",
    "    tags: list[str]\n",
    "    site: str = \"en.wikipedia.org\"\n",
    "\n",
    "def is_revert(revision: PageRevision, original_edit, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Indicate if the edit is a revert.\n",
    "\n",
    "    Uses sentence-transformers to semantically compare the comment with\n",
    "    predefined revert indicator phrases.\n",
    "\n",
    "    Returns True if the comment is likely a revert.\n",
    "    \"\"\"\n",
    "\n",
    "    log = logger.getChild(\"is_revert\")\n",
    "    log.debug(\"Checking if revision %r is a revert\", revision)\n",
    "\n",
    "    comment = revision.get(\"comment\", \"\")\n",
    "    # Remove comment (/* ... */) from the comment that is used to indicate section\n",
    "    comment = re.sub(r'/\\* .*? \\*/', '', comment)\n",
    "\n",
    "    if not comment:\n",
    "        log.debug(\"Revision %r has no comment, skipping\", revision)\n",
    "        return False\n",
    "\n",
    "    # If the revision ID or username is not mentioned, it's not likely a revert. \n",
    "    username = original_edit.get(\"user\", \"\")\n",
    "    revision_id_str = str(original_edit.get(\"revid\"))\n",
    "\n",
    "    rev_pattern = r'\\b' + re.escape(revision_id_str) + r'\\b'\n",
    "    username_pattern = r'\\b' + re.escape(username) + r'\\b'\n",
    "    \n",
    "    has_revision_id = bool(re.search(rev_pattern, comment))\n",
    "    user_mentioned = bool(re.search(username_pattern, comment))\n",
    "\n",
    "    if not any((has_revision_id, user_mentioned)):\n",
    "        log.debug(\"Revision ID %s or username %r not mentioned in comment: %r\", revision_id_str,  username, comment)\n",
    "        return False\n",
    "\n",
    "    formatted_revert_phrases = [s.format(**original_edit) for s in revert_phrases]\n",
    "\n",
    "    # Encode the revert phrases.\n",
    "    revert_embeddings = model.encode(formatted_revert_phrases, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    # Encode the comment.\n",
    "    comment_embedding = model.encode(comment, convert_to_tensor=True, show_progress_bar=False)\n",
    "    # Compute (cosine) similarity with the pre-encoded revert phrases.\n",
    "    cosine_scores = model.similarity(comment_embedding, revert_embeddings)\n",
    "\n",
    "    # Use the maximum similarity score as the indicator.\n",
    "    # Using for-loop to be able to log the similarity score for each pattern.\n",
    "    #max_score = cosine_scores.max().item()\n",
    "    max_score = 0.0\n",
    "    for i, score in enumerate(cosine_scores[0]):\n",
    "        max_score = max(score, max_score)\n",
    "        log.debug(f\" {max_score:.2f} - Pattern {formatted_revert_phrases[i]!r} match to {comment!r}\")\n",
    "\n",
    "    # Set a threshold for similarity (this needs to be fine-tuned).\n",
    "    if max_score >= threshold:\n",
    "        log.info(\"Revision %r is a revert\", revision)\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'revid': 1254954349,\n",
       "  'parentid': 1254925063,\n",
       "  'user': 'Binksternet',\n",
       "  'timestamp': '2024-11-02T13:09:40Z',\n",
       "  'comment': 'Reverted 1 edit by [[Special:Contributions/31.217.4.176|31.217.4.176]] ([[User talk:31.217.4.176|talk]]) to last revision by 162 etc.',\n",
       "  'tags': ['mw-undo', 'twinkle', 'mw-reverted'],\n",
       "  'pageid': '49079438'},\n",
       " {'revid': 1254925063,\n",
       "  'parentid': 1254811923,\n",
       "  'user': '31.217.4.176',\n",
       "  'anon': '',\n",
       "  'timestamp': '2024-11-02T10:02:53Z',\n",
       "  'comment': '',\n",
       "  'tags': ['mobile edit',\n",
       "   'mobile web edit',\n",
       "   'visualeditor',\n",
       "   'mw-reverted',\n",
       "   'disambiguator-link-added'],\n",
       "  'pageid': '49079438'},\n",
       " {'revid': 1254811923,\n",
       "  'parentid': 1254792543,\n",
       "  'user': '162 etc.',\n",
       "  'timestamp': '2024-11-01T20:19:03Z',\n",
       "  'comment': 'Undid revision [[Special:Diff/1254792543|1254792543]] by [[Special:Contributions/31.217.4.176|31.217.4.176]] ([[User talk:31.217.4.176|talk]])',\n",
       "  'tags': ['mw-undo', 'wikieditor'],\n",
       "  'pageid': '49079438'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_last_revisions(page_title, limit=20) -> list[PageRevision]:\n",
    "    \"\"\"\n",
    "    Fetches the last `limit` revisions for the given Wikipedia page title.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'titles': page_title,\n",
    "        'rvlimit': limit,\n",
    "        'rvprop': 'ids|timestamp|user|comment|tags',\n",
    "    }\n",
    "\n",
    "    data = wpapi(params)\n",
    "    pages = data.get('query', {}).get('pages', {})\n",
    "    revisions = []\n",
    "    # The query returns a dictionary keyed by pageid\n",
    "    for page_id, page in pages.items():\n",
    "        if \"missing\" in page:\n",
    "            print(f\"The page '{page_title}' does not exist on Wikipedia.\")\n",
    "            return None\n",
    "        revisions = page.get('revisions', [])\n",
    "        revisions.extend([PageRevision(**rev, pageid=page_id) for rev in revisions])\n",
    "    return revisions\n",
    "\n",
    "page_edits = fetch_last_revisions(\"Sofi_Tukker\", limit=15)\n",
    "display(page_edits[-3:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 3 contributions by '162 etc.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'userid': 41625025,\n",
       "  'user': '162 etc.',\n",
       "  'pageid': 26840440,\n",
       "  'revid': 1276984291,\n",
       "  'parentid': 1260787574,\n",
       "  'ns': 0,\n",
       "  'title': 'Blue Blood',\n",
       "  'timestamp': '2025-02-21T23:11:49Z',\n",
       "  'comment': 'Cleanup',\n",
       "  'tags': ['wikieditor']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_user_contributions(username: str, limit=100) -> list[PageRevision]:\n",
    "    \"\"\"\n",
    "    Fetches up to `limit` contributions made by the given user.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"usercontribs\",\n",
    "        \"ucuser\": username,\n",
    "        \"uclimit\": limit,\n",
    "        \"ucprop\": \"ids|title|timestamp|comment|tags\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params=params)\n",
    "\n",
    "    # Only consider the page edits\n",
    "    return [PageRevision(edit) for edit in data.get(\"query\", {}).get(\"usercontribs\", []) if edit.get(\"ns\", 0) == 0]\n",
    "\n",
    "username = page_edits[-1]['user']\n",
    "user_contributions = fetch_user_contributions(username, limit=3)\n",
    "print(f\"Last 3 contributions by {username!r}\")\n",
    "display(user_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking edit: {'revid': 1254811923, 'parentid': 1254792543, 'user': '162 etc.', 'timestamp': '2024-11-01T20:19:03Z', 'comment': 'Undid revision [[Special:Diff/1254792543|1254792543]] by [[Special:Contributions/31.217.4.176|31.217.4.176]] ([[User talk:31.217.4.176|talk]])', 'tags': ['mw-undo', 'wikieditor'], 'pageid': '49079438'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "def get_next_revisions(revision: PageRevision, n) -> list[PageRevision]:\n",
    "    \"\"\"\n",
    "    Fetches the next `n` revisions after the given timestamp on the specified page.\n",
    "    \"\"\"\n",
    "\n",
    "    revision_timestamp = revision['timestamp']\n",
    "\n",
    "    dt = datetime.strptime(revision_timestamp, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    dt_next = dt + timedelta(seconds=1)\n",
    "    start_timestamp = dt_next.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"pageids\": revision['pageid'],\n",
    "        \"rvstart\": start_timestamp,\n",
    "        \"rvdir\": \"newer\",\n",
    "        \"rvlimit\": n,\n",
    "        \"rvprop\": \"ids|timestamp|comment|tags\"\n",
    "    }\n",
    "    data = wpapi(params)\n",
    "    logger.debug(\"Next revisions: %r\", data)\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    revisions_list = []\n",
    "    for pid, revision in pages.items():\n",
    "        revisions = revision.get(\"revisions\", [])\n",
    "        revisions_list.extend([PageRevision(**rev, pageid=pid) for rev in revisions])\n",
    "    return revisions_list\n",
    "\n",
    "\n",
    "def check_contribution_reverted(revision: PageRevision, num_later_edits=10) -> List[bool | PageRevision]:\n",
    "    \"\"\"\n",
    "    Has the contribution been reverted?\n",
    "\n",
    "    Checks if a particular contribution was reverted by examining the next\n",
    "    `num_later_edits` revisions on the page. If any of those revisions' comments\n",
    "    indicate a revert targeting the given username or revision, returns True.\n",
    "\n",
    "    Returns list of N>0 if the edit is reverted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the edit has the \"mw-reverted\" tag.\n",
    "    cx_tags = MW_REVERTED_TAGS & set(revision.get(\"tags\", []))\n",
    "    logger.debug(\"Revision tags: %r\", revision.get(\"tags\", []))\n",
    "    if len(cx_tags):\n",
    "        logger.info(\"[REVERTED] Revision %d is tagged as reverted with tag(s) %r\", revision['revid'], cx_tags)\n",
    "        return [True]\n",
    "\n",
    "    # Check the follow-up revisions if they mention this edit.\n",
    "    next_revs = get_next_revisions(revision, num_later_edits)\n",
    "    logger.debug(\"Number of following revisions: %d\", len(next_revs))\n",
    "\n",
    "    r = []\n",
    "    for followup_revision in next_revs:\n",
    "        comment = followup_revision.get(\"comment\", \"\")\n",
    "\n",
    "        logger.debug(f\"Checking revision {followup_revision['revid']} with comment: {comment!r}\")\n",
    "\n",
    "        if is_revert(followup_revision, revision):\n",
    "            # TODO: We should recurse to see, if reverting revision has been reverted\n",
    "            logger.info(\"[REVERTED] Comment %r is reverted by %r (revid:%d)\", revision['comment'], comment, followup_revision['revid'])\n",
    "            r.append(followup_revision)\n",
    "    return r\n",
    "\n",
    "earliest_contrib = page_edits[-1]\n",
    "print(\"Checking edit:\", earliest_contrib)\n",
    "\n",
    "reverts = list(check_contribution_reverted(earliest_contrib))\n",
    "\n",
    "display(reverts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[REVERTED] Revision 1256966548 is tagged as reverted with tag(s) {'mw-reverted'}\n",
      "INFO:__main__:[REVERTED] Revision 1256962846 is tagged as reverted with tag(s) {'mw-reverted'}\n",
      "INFO:__main__:[REVERTED] Revision 1256962795 is tagged as reverted with tag(s) {'mw-reverted'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revert rate for 2A05:4F44:1701:2500:D010:2C76:10A4:52C7: 100.00% (3/3)\n"
     ]
    }
   ],
   "source": [
    "# Final run - Collect edits, collect editors from the edits, and check how many of the lastest edits by the editor is reverted.\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class RevertStats(NamedTuple):\n",
    "    rate: float\n",
    "    total: int\n",
    "    reverted: int\n",
    "\n",
    "def compute_user_revert_rate(username: str, contrib_limit: int = 100, later_edits_to_check: int = 10) -> RevertStats:\n",
    "    \"\"\"\n",
    "    Computes the revert rate for a user by checking up to `contrib_limit`\n",
    "    of their contributions. For each contribution, it checks the next\n",
    "    `later_edits_to_check` revisions to see if it was reverted.\n",
    "    \n",
    "    Returns a tuple: (revert_rate, total_checked, total_reverted).\n",
    "    \"\"\"\n",
    "    contributions = fetch_user_contributions(username, limit=contrib_limit)\n",
    "    if not contributions:\n",
    "        return RevertStats(None, 0, 0)\n",
    "    total = 0\n",
    "    reverted = 0\n",
    "    for contrib in contributions:\n",
    "        total += 1\n",
    "        if check_contribution_reverted(contrib, num_later_edits=later_edits_to_check):\n",
    "            reverted += 1\n",
    "        # time.sleep(0.2)\n",
    "    rate = (reverted / total) if total > 0 else 0\n",
    "    return RevertStats(rate, total, reverted)\n",
    "\n",
    "username = \"2A05:4F44:1701:2500:D010:2C76:10A4:52C7\"\n",
    "rate, total, reverted = compute_user_revert_rate(username, 10)\n",
    "print(f\"Revert rate for {username}: {rate:.2%} ({reverted}/{total})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rank the editors by revert rate\n",
    "\n",
    "editors = set([edit['user'] for edit in page_edits])\n",
    "\n",
    "# revert_rates = {editor: compute_user_revert_rate(editor) for editor in editors}\n",
    "\n",
    "# sorted_revert_rates = sorted(revert_rates.items(), key=lambda x: x[1].rate, reverse=True)\n",
    "\n",
    "# for editor, stats in sorted_revert_rates:\n",
    "#     print(f\"{editor}: {stats.rate:.2%} ({stats.reverted}/{stats.total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group based power-level\n",
    "\n",
    "Wikipedia uses RBAC (Role Based Access Control) to determine the rights of the user. In theory, the rights should have been granted by their merit. Use the groups as indicator of \"power-level\" of the user. More powerful the user is, the more likely they are to be trusted – according to wikipedia community.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:User 'Jayson Cesar Bautista' is blocked: '[[WP:SOCK|Sock puppetry]]'\n",
      "INFO:__main__:IP '2A05:4F44:1701:2500:D010:2C76:10A4:52C7' is blocked: '[[WP:Disruptive editing|Disruptive editing]], block evasion, vandalism, see 2A05:4F44:1704:D500:0:0:0:0/64'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusikBot II: 111\n",
      "Duckmather: 21\n",
      "Binksternet: 21\n",
      "GreenC bot: 11\n",
      "Chetsford: 101\n",
      "Arjayay: 21\n",
      "31.217.4.176: 0\n",
      "162 etc.: 21\n",
      "107.116.79.140: 0\n",
      "174.92.221.85: 0\n",
      "2A05:4F44:1701:2500:D010:2C76:10A4:52C7: -10000\n",
      "Jayson Cesar Bautista: -10000\n"
     ]
    }
   ],
   "source": [
    "# Map of groups to an power level.\n",
    "\n",
    "from collections import OrderedDict\n",
    "import ipaddress\n",
    "from typing import Iterable\n",
    "\n",
    "# Group to use if the IP address is blocked.\n",
    "USER_BLOCKED_GROUP = \"blocked\"\n",
    "\n",
    "GROUP_TRUST_LEVELS = {\n",
    "    \"banned\": -10000,\n",
    "    \"blocked\": -10000,\n",
    "    \"bot\": 10,\n",
    "    \"autoconfirmed\": 1,\n",
    "    \"extendedconfirmed\": 20,\n",
    "    \"rollback\": 50,\n",
    "    \"sysop\": 100,\n",
    "    \"bureaucrat\": 400,\n",
    "    \"checkuser\": 350,\n",
    "    \"oversight\": 350,\n",
    "    \"steward\": 350,\n",
    "}\n",
    "\n",
    "def check_ip_blocked(ip: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the IP address is blocked on Wikipedia.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"blocks\",\n",
    "        \"bkip\": ip,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params)\n",
    "    block = data.get(\"query\", {}).get(\"blocks\", [])\n",
    "    if bool(block):\n",
    "        logger.info(\"IP %r is blocked: %r\", ip, block[0].get(\"reason\", \"\"))\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_power_levels(users: Iterable[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get the power levels of the given users.\n",
    "\n",
    "    The power level is a sum of the trust levels of the groups the user is in. Higher\n",
    "    power level indicates more trust. The trust levels are defined in the GROUP_TRUST_LEVELS\n",
    "    dictionary.\n",
    "\n",
    "    :param users: List of usernames or IP addresses.\n",
    "    :return: List of power levels for each user.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the user map with 0 trust level.\n",
    "    user_map = OrderedDict((u, 0) for u in users)\n",
    "\n",
    "    # Separate IP addresses from usernames, they need to be queried differently.\n",
    "    ips, usernames = [], []\n",
    "    for entry in users:\n",
    "        try:\n",
    "            if ipaddress.ip_address(entry):\n",
    "                ips.append(str(entry))\n",
    "        except ValueError:\n",
    "            usernames.append(entry)\n",
    "\n",
    "    # Query the user groups and blockinfo for the given usernames.\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"users\",\n",
    "        \"ususers\": \"|\".join(usernames),\n",
    "        \"usprop\": \"groups|blockinfo\",\n",
    "    }\n",
    "    data = wpapi(params)\n",
    "\n",
    "    for user_info in data.get(\"query\", {}).get(\"users\", []):\n",
    "        # If user is tagged as \"invalid\"\n",
    "        if 'invalid' in user_info:\n",
    "            logger.warning(\"User %r is invalid: %r\", user_info.get('name', \"\"), user_info.get('invalidreason', \"\"))\n",
    "            continue\n",
    "\n",
    "        total_trust = 0\n",
    "        user = user_info['name']\n",
    "        groups = user_info.get(\"groups\", [])\n",
    "\n",
    "        if user_info.get(\"blockid\", None) is not None:\n",
    "            logger.info(\"User %r is blocked: %r\", user, user_info.get('blockreason', \"\"))\n",
    "            groups.append(USER_BLOCKED_GROUP)\n",
    "\n",
    "        for group in set(groups):\n",
    "            group_key = group.lower()  # Ensure case-insensitive matching.\n",
    "            trust = GROUP_TRUST_LEVELS.get(group_key, 0)  # Default to 0 if not mapped.\n",
    "            total_trust += trust\n",
    "        user_map[user] = total_trust\n",
    "\n",
    "    # Check if the IP address is blocked.\n",
    "    for ip in ips:\n",
    "        if check_ip_blocked(ip):\n",
    "            user_map[ip] += GROUP_TRUST_LEVELS[USER_BLOCKED_GROUP]\n",
    "\n",
    "    return list(user_map.values())\n",
    "\n",
    "def rank_by_power_level(users: Iterable[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Rank the users by their power levels.\n",
    "\n",
    "    :param users: List of usernames or IP addresses.\n",
    "    :return: List of usernames sorted by power level.\n",
    "    \"\"\"\n",
    "    power_levels = get_power_levels(users)\n",
    "    return [u for p, u in sorted(zip(power_levels, users), reverse=True) if p > 0]\n",
    "\n",
    "level_check_users = list(editors) + [\"Jayson Cesar Bautista\"]\n",
    "\n",
    "power_levels = get_power_levels(level_check_users)\n",
    "for editor, power in zip(level_check_users, power_levels):\n",
    "    print(f\"{editor}: {power}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review log\n",
    "\n",
    "Rank the revisions based on their review status. Some wikipedias employ review flags to indicate that the revision has been reviewed by a trusted user. Example from Finnish wikipedia: https://fi.wikipedia.org/wiki/Ohje:Sivujen_arviointi\n",
    "\n",
    "Having a review flag is NOT a guarantee that the information is correct, but it can be used as an indicator that the revision has no obvious vandalism.\n",
    "\n",
    "Revisions with flags are preferred, and reviews from other users are ranked above having a \"trusted\" status. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'revid': 22889573,\n",
       "  'parentid': 22889569,\n",
       "  'user': 'Fribaaja',\n",
       "  'timestamp': '2024-12-11T13:24:01Z',\n",
       "  'flagged': {'user': 'Osmo Lundell',\n",
       "   'timestamp': '2025-02-20T21:58:19Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22638561,\n",
       "  'parentid': 22595899,\n",
       "  'user': '87.95.32.17',\n",
       "  'anon': '',\n",
       "  'timestamp': '2024-09-16T19:46:57Z',\n",
       "  'flagged': {'user': 'BladeJ',\n",
       "   'timestamp': '2024-10-06T19:56:57Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22595899,\n",
       "  'parentid': 22595855,\n",
       "  'user': 'Kuosmanono',\n",
       "  'timestamp': '2024-08-25T22:41:38Z',\n",
       "  'flagged': {'user': 'BladeJ',\n",
       "   'timestamp': '2024-09-07T13:21:10Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22358375,\n",
       "  'parentid': 22273183,\n",
       "  'user': 'SaMSUoM',\n",
       "  'timestamp': '2024-04-29T16:50:05Z',\n",
       "  'flagged': {'user': 'Seegge',\n",
       "   'timestamp': '2024-07-28T20:44:51Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22079023,\n",
       "  'parentid': 22037750,\n",
       "  'user': '176.72.103.70',\n",
       "  'anon': '',\n",
       "  'timestamp': '2023-12-30T10:26:47Z',\n",
       "  'flagged': {'user': 'Elastul',\n",
       "   'timestamp': '2024-01-01T08:22:44Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22595855,\n",
       "  'parentid': 22358375,\n",
       "  'user': 'SaMSUoM',\n",
       "  'timestamp': '2024-08-25T22:17:42Z',\n",
       "  'flagged': {'user': 'SaMSUoM',\n",
       "   'timestamp': '2024-08-25T22:17:42Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22148856,\n",
       "  'parentid': 22146625,\n",
       "  'user': 'Puppe100',\n",
       "  'timestamp': '2024-01-29T15:32:53Z',\n",
       "  'flagged': {'user': 'Puppe100',\n",
       "   'timestamp': '2024-01-29T15:33:19Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22145999,\n",
       "  'parentid': 22145278,\n",
       "  'user': 'Elastul',\n",
       "  'timestamp': '2024-01-28T15:22:21Z',\n",
       "  'flagged': {'user': 'Elastul',\n",
       "   'timestamp': '2024-01-28T15:22:21Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'},\n",
       " {'revid': 22037750,\n",
       "  'parentid': 21747700,\n",
       "  'user': 'Unkka T. Kumiankka',\n",
       "  'timestamp': '2023-12-09T14:02:05Z',\n",
       "  'flagged': {'user': 'Unkka T. Kumiankka',\n",
       "   'timestamp': '2023-12-09T14:02:05Z',\n",
       "   'level': 0,\n",
       "   'level_text': 'stable',\n",
       "   'tags': {'accuracy': 1}},\n",
       "  'pageid': '1610614'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REVIEWED_TAGS = {\"stable\", \"reviewed\"}\n",
    "\n",
    "def get_page_revisions_with_review(page_title, rvlimit=20):\n",
    "    \"\"\"\n",
    "    Retrieves the latest revisions for a page along with flagged (review) info.\n",
    "   \n",
    "    Revisions with \"stable\" are ranked above non-reviewed revisions, and revisions made by other users are ranked above\n",
    "    the user's own revisions.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": page_title,\n",
    "        \"rvlimit\": rvlimit,\n",
    "        \"rvdir\": \"older\",\n",
    "        \"rvprop\": \"ids|timestamp|user|flagged\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params, site=\"fi.wikipedia.org\")\n",
    "\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    r = []\n",
    "    for page_id, page in pages.items():\n",
    "        revisions = page.get(\"revisions\", [])\n",
    "        \n",
    "        for rev in revisions:\n",
    "            score = 0\n",
    "            flagged = rev.get(\"flagged\", {})\n",
    "            if not flagged: continue\n",
    "            if flagged.get(\"level_text\", \"\") in REVIEWED_TAGS:\n",
    "                score += 1\n",
    "                if rev[\"user\"] != flagged.get(\"user\", None):\n",
    "                    # If the revision is reviewed and the reviewer is different from the editor, give extra points.\n",
    "                    score += 1\n",
    "\n",
    "            r.append((score, PageRevision(rev, pageid=page_id)))\n",
    "    # Sort by score in descending order.\n",
    "    r.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [rev for score, rev in r]\n",
    "\n",
    "page_title = \"Niklas_Anttila\"\n",
    "page_revisions = get_page_revisions_with_review(page_title)\n",
    "display(page_revisions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "Using the basis presented in the paper \"Context-aware Detection of Sneaky Vandalism on Wikipedia across Multiple Languages\", use part-of-speech tagging to detect \"sneaky vandalism\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "Compute a changed sentences from the revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_revision(oldid: int, site=\"en.wikipedia.org\") -> PageRevision:\n",
    "    \"\"\"\n",
    "    Load the revision by :param oldid: and return the :class:`PageRevision`.\n",
    "    \"\"\"\n",
    "    oldid = int(oldid)\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"revids\": oldid,\n",
    "        \"rvprop\": \"ids|timestamp|user|comment|tags\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    data = wpapi(params, site=site)\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for page_id, page in pages.items():\n",
    "        revisions = page.get(\"revisions\", [])\n",
    "\n",
    "        assert len(revisions) == 1, f\"Expected 1 revision, got {len(revisions)}\"\n",
    "\n",
    "        return PageRevision(revisions[0], pageid=page_id, site=site)\n",
    "\n",
    "    raise ValueError(f\"Revision with oldid {oldid!r} not found\")\n",
    "\n",
    "def pervious_notable_revision(revision: PageRevision) -> PageRevision:\n",
    "    \"\"\"\n",
    "    Get previous notable revision\n",
    "\n",
    "    Vandals might do a multiple edits in succession, so find a earlier revision that is from a different user, or a day before.\n",
    "    \"\"\"\n",
    "    MAX_DAYS = timedelta(days=1)\n",
    "    start_timestamp = datetime.strptime(revision['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\") + timedelta(seconds=1)\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"pageids\": revision['pageid'],\n",
    "        \"rvstart\": start_timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"rvdir\": \"older\",\n",
    "        \"rvlimit\": 20,\n",
    "        \"rvprop\": \"ids|timestamp|comment|tags|user\"\n",
    "    }\n",
    "\n",
    "    data = wpapi(params, site=revision['site'])\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for pid, page in pages.items():\n",
    "        revisions = page.get(\"revisions\", [])\n",
    "        for rev in revisions:\n",
    "            if rev['user'] != revision['user']:\n",
    "                return PageRevision(rev, pageid=pid)\n",
    "            rev_timestamp = datetime.strptime(rev['timestamp'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            if rev_timestamp > (start_timestamp + MAX_DAYS):\n",
    "                return PageRevision(rev, pageid=pid, site=revision['site'])\n",
    "    logger.warning(\"No notable revision found for %d (revid %r)\", revision[\"pageid\"], revision['revid'])\n",
    "    return None\n",
    "\n",
    "current_rev = load_revision(23006899, site=\"fi.wikipedia.org\")\n",
    "prev_rev = pervious_notable_revision(current_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from meri.scraper import get_user_agent\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"trafilatura\").setLevel(logging.DEBUG)\n",
    "\n",
    "def wp_page(revision: PageRevision) -> str:\n",
    "    \"\"\"\n",
    "    Fetch the Wikipedia page content for the given revision.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\"oldid\": revision[\"revid\"]}\n",
    "\n",
    "    params.update({\n",
    "        \"action\": \"parse\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"displaytitle|text|headhtml\",\n",
    "        \"disablelimitreport\": True,\n",
    "        \"disableeditsection\": True,\n",
    "        \"mobileformat\": False,\n",
    "        \"contentmodel\": \"wikitext\",\n",
    "        \"disabletoc\": True,\n",
    "    })\n",
    "\n",
    "    site = \"fi.wikipedia.org\"\n",
    "    data = wpapi(params, site=site)\n",
    "\n",
    "    perm_url = \"https://{site}/w/index.php?title={title}&oldid={revid}\".format(\n",
    "        site=site,\n",
    "        **data[\"parse\"]\n",
    "    )\n",
    "\n",
    "    html = \"\\n\".join([\n",
    "        data[\"parse\"][\"headhtml\"][\"*\"],\n",
    "        '<main id=\"content\" class=\"mw-body\"><div id=\"bodyContent\" class=\"content\">',\n",
    "        f'<h1>{data['parse']['displaytitle']}</h1>',\n",
    "        data[\"parse\"][\"text\"][\"*\"],\n",
    "        '</div></main>',\n",
    "        '</body></html>'\n",
    "    ])\n",
    "\n",
    "    from trafilatura import load_html\n",
    "    from trafilatura.settings import DEFAULT_CONFIG\n",
    "    from trafilatura.htmlprocessing import prune_unwanted_nodes\n",
    "    from lxml.etree import XPath\n",
    "    config = deepcopy(DEFAULT_CONFIG)\n",
    "    config[\"DEFAULT\"].setdefault(\"USER_AGENTS\", get_user_agent())\n",
    "\n",
    "    doc = load_html(html)\n",
    "\n",
    "    # Convert <a> tags to text\n",
    "    for a in doc.xpath(\"//a\"):\n",
    "        a.tag = \"span\"\n",
    "        a.text = a.text or \"\"\n",
    "        a.attrib.clear()\n",
    "\n",
    "    # Prune unwanted nodes\n",
    "    doc = prune_unwanted_nodes(doc, [XPath(x) for x in [\n",
    "        \"//script\", \"//noscript\", \"//style\", \"//link\", \"//meta\", \"//form\", \"//input\", \"//button\",\n",
    "        '//*[contains(@class, \"noprint\") or contains(@class, \"ambox-notice\")]',  # Remove \"noprint\" classed content\n",
    "    ]])\n",
    "\n",
    "    #from lxml import etree\n",
    "    #print(etree.tostring(doc).decode())\n",
    "\n",
    "    # TODO: Use trafilatura bare_extraction\n",
    "    #from trafilatura import bare_extraction\n",
    "    from trafilatura.htmlprocessing import tree_cleaning, convert_tags\n",
    "    from trafilatura.core import Extractor\n",
    "    from trafilatura.xml import xmltotxt\n",
    "    options = Extractor(\n",
    "        config=config,\n",
    "        output_format=\"txt\",\n",
    "        formatting=False,\n",
    "        links=False,\n",
    "        images=False,\n",
    "        tables=True,\n",
    "        comments=False,\n",
    "    )\n",
    "\n",
    "    doc = tree_cleaning(doc, options)\n",
    "    doc = convert_tags(doc, options, perm_url)\n",
    "    txt = xmltotxt(doc.body, False)\n",
    "\n",
    "    # Cleaup hacks\n",
    "    # Remove citation links\n",
    "    txt = re.sub(r'\\[\\s*\\d+\\]', '', txt)\n",
    "    # Clean whitespace before word boundaries, left by some inline tags from trafilatura (e.g. <i> <b>)\n",
    "    txt = re.sub(r'\\s+([.,;:!?)])', r'\\1', txt)\n",
    "\n",
    "    return txt\n",
    "\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "page_text = wp_page(current_rev)\n",
    "\n",
    "#display_markdown(page_text[:300], raw=True)\n",
    "#print(page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Aligned sentences:** 144"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Vandalized:** Tällöin muita henkilöryhmiä ei kuitenkaan välttämättä kohdella reilusti."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Clean:** N/A"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Vandalized:** Tälle vastakohtaisesti monet organisaatiot haluavat tukea ihmisiä vain saavutuksiin perustavalla (merit-based) tavalla, jotta ketään ei tarvitsisi syrjiä."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Clean:** DEI-politiikka voi tarkoittaa vain työvoiman monimuotoisuutta, mutta se voi tarkoittaa myös palkkojen ja muiden etujen tasapuolisuutta sekä yhtäläisiä vaikutusmahdollisuuksia."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Vandalized:** DEI-politiikka voi tarkoittaa työvoiman monimuotoisuutta, mutta se voi tarkoittaa myös palkkojen ja muiden etujen kohdistamista tietyille vähemmistöryhmille sekä vähemmistöryhmien parannettuja vaikutusmahdollisuuksia."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Clean:** DEI-politiikkaa on yleistynyt myös voittoa tavoittelemattomalle sektorille ja avoimen lähdekoodin projekteissa."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import difflib\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "import spacy\n",
    "import spacy.tokens\n",
    "#spacy.cli.download(\"fi_core_news_md\")\n",
    "nlp = spacy.load(\"fi_core_news_md\")\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Handles text preprocessing, sentence segmentation, token-level alignment,\n",
    "    and feature extraction using spaCy.\n",
    "    \"\"\"\n",
    "\n",
    "    NORMAL = 'N'\n",
    "    VANDALIZED = 'V'\n",
    "\n",
    "    class WordFeature(TypedDict):\n",
    "        \"\"\"\n",
    "        Features for a single token in a sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        bias: float  = 1.0\n",
    "        token_lower: str\n",
    "        token_lemma: str\n",
    "        token_pos: str\n",
    "        token_tag: str\n",
    "        token_shape: str\n",
    "        is_alpha: bool\n",
    "        is_digit: bool\n",
    "\n",
    "        # Features of previous token\n",
    "        prev_token_lower: Optional[str] = None\n",
    "        prev_token_pos: Optional[str] = None\n",
    "        prev_token_tag: Optional[str] = None\n",
    "\n",
    "        # Features of next token\n",
    "        next_token_lower: Optional[str] = None\n",
    "        next_token_pos: Optional[str] = None\n",
    "        next_token_tag: Optional[str] = None\n",
    "\n",
    "        # Special features for beginning and end of sentence\n",
    "        EOS: bool = False\n",
    "        BOS: bool = False\n",
    "\n",
    "    nlp: spacy.Language\n",
    "\n",
    "\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "\n",
    "\n",
    "    def extract_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Uses spaCy to segment markdown text into sentences.\n",
    "        \"\"\"\n",
    "        # Split into paragraphs, at least the Finnish model does not handle newlines well\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\")]\n",
    "        sentences = []\n",
    "        for para in paragraphs:\n",
    "            doc = self.nlp(para)\n",
    "            sentences.extend([sent.text.strip() for sent in doc.sents if sent.text.strip()])\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def align_documents(self, clean_text: str, vandal_text: str) -> List[Tuple[str, Optional[str]]]:\n",
    "        \"\"\"\n",
    "        Align sentences between the non-vandalized (clean) and vandalized documents.\n",
    "        Returns a list of tuples: (vandalized_sentence, corresponding_clean_sentence or None).\n",
    "        If a sentence in the vandalized version has no aligned counterpart, the clean sentence is None.\n",
    "        \"\"\"\n",
    "        clean_sents = self.extract_sentences(clean_text)\n",
    "        vandal_sents = self.extract_sentences(vandal_text)\n",
    "\n",
    "        matcher = difflib.SequenceMatcher(None, clean_sents, vandal_sents)\n",
    "        aligned = []\n",
    "\n",
    "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "            if tag in ('equal', 'replace'):\n",
    "                # Aligned sentences; pair each sentence one-to-one\n",
    "                for idx_clean, idx_vandal in zip(range(i1, i2), range(j1, j2)):\n",
    "                    aligned.append((vandal_sents[idx_vandal], clean_sents[idx_clean]))\n",
    "            elif tag == 'insert':\n",
    "                # In vandalized text, sentences that do not match clean version.\n",
    "                # Mark these as fully vandalized.\n",
    "                for idx in range(j1, j2):\n",
    "                    # For 'replace' we could use the clean sentence too, but for simplicity we mark entire sentence as vandalized.\n",
    "                    aligned.append((vandal_sents[idx], None))\n",
    "            elif tag == 'delete':\n",
    "                # Sentences missing from vandalized version: ignore.\n",
    "                continue\n",
    "        return aligned\n",
    "\n",
    "\n",
    "    def token_level_label(self, v_sent: str, c_sent: Optional[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Given a vandalized sentence and its corresponding clean sentence (if available),\n",
    "        return a list of (token, label) pairs.\n",
    "        \n",
    "        If c_sent is provided, use difflib.SequenceMatcher on token lists to label tokens:\n",
    "        - tokens inserted or replaced (relative to c_sent) are labeled 'V'\n",
    "        - tokens unchanged are labeled 'N'\n",
    "        \n",
    "        If c_sent is None (i.e. sentence is extra in vandalized text), label all tokens as 'V'.\n",
    "        \"\"\"\n",
    "        doc_v = self.nlp(v_sent)\n",
    "        v_tokens = [token.text for token in doc_v]\n",
    "        \n",
    "        if c_sent is None:\n",
    "            # Entire sentence considered vandalized\n",
    "            return [(token.text, self.VANDALIZED) for token in doc_v]\n",
    "        \n",
    "        doc_c = self.nlp(c_sent)\n",
    "        c_tokens = [token.text for token in doc_c]\n",
    "        \n",
    "        matcher = difflib.SequenceMatcher(None, c_tokens, v_tokens)\n",
    "        labels = [self.NORMAL] * len(v_tokens)  # default label: normal\n",
    "        for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "            if tag in ('replace', 'insert'):\n",
    "                print(f\"Tag: {tag}, i1: {i1}, i2: {i2}, j1: {j1}, j2: {j2}\")\n",
    "                for j in range(j1, j2):\n",
    "                    labels[j] = self.VANDALIZED\n",
    "            # 'delete' doesn't affect tokens in the vandalized sentence.\n",
    "            else:\n",
    "                print(f\"Unknown tag: {tag}\")\n",
    "        return [(token.text, labels[idx]) for idx, token in enumerate(doc_v)]\n",
    "\n",
    "\n",
    "    def word2features(self, sent: List[Any], i: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract features for the token at index i in a sentence.\n",
    "        Includes token features and context from neighboring tokens.\n",
    "        \"\"\"\n",
    "        token = sent[i]\n",
    "\n",
    "        features = self.WordFeature(\n",
    "            bias=1.0,\n",
    "            token_lower=token.text.lower(),\n",
    "            token_lemma=token.lemma_,\n",
    "            token_pos=token.pos_,\n",
    "            token_tag=token.tag_,\n",
    "            token_shape=token.shape_,\n",
    "            is_alpha=token.is_alpha,\n",
    "            is_digit=token.is_digit,\n",
    "        )\n",
    "\n",
    "        # Add features from neighboring tokens\n",
    "        if i > 0:\n",
    "            token_prev = sent[i - 1]\n",
    "            features.update({\n",
    "                'prev_token_lower': token_prev.text.lower(),\n",
    "                'prev_token_pos': token_prev.pos_,\n",
    "                'prev_token_tag': token_prev.tag_,\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True  # Beginning of sentence\n",
    "        if i < len(sent) - 1:\n",
    "            token_next = sent[i + 1]\n",
    "            features.update({\n",
    "                'next_token_lower': token_next.text.lower(),\n",
    "                'next_token_pos': token_next.pos_,\n",
    "                'next_token_tag': token_next.tag_,\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True  # End of sentence\n",
    "        return features\n",
    "\n",
    "\n",
    "    def sent2features(self, sent: List[Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Converts a list of spaCy tokens into a list of feature dictionaries.\n",
    "        \"\"\"\n",
    "        return [self.word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "# Load vandalized version\n",
    "vandalized_rev = load_revision(23006899, site=\"fi.wikipedia.org\")\n",
    "\n",
    "# Find unvandalized version\n",
    "clean_rev = pervious_notable_revision(vandalized_rev)\n",
    "\n",
    "vandalized_text = wp_page(vandalized_rev)\n",
    "clean_text = wp_page(clean_rev)\n",
    "\n",
    "fe = FeatureExtractor(nlp)\n",
    "aligned = fe.align_documents(clean_text, vandalized_text)\n",
    "display_markdown(f\"**Aligned sentences:** {len(aligned)}\", raw=True)\n",
    "\n",
    "for vandal_sent, clean_sent in aligned:\n",
    "    if vandal_sent == clean_sent:\n",
    "        #display_markdown(f\"**Identical:** {vandal_sent}\", raw=True)\n",
    "        continue\n",
    "    display_markdown(f\"**Vandalized:** {vandal_sent}\", raw=True)\n",
    "    if clean_sent:\n",
    "        display_markdown(f\"**Clean:** {clean_sent}\", raw=True)\n",
    "    else:\n",
    "        display_markdown(\"**Clean:** N/A\", raw=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
